{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allX, ally = load_mnist('Weights/Ques2/', kind='train')\n",
    "allX_2, ally_2 = load_mnist('Weights/Ques2/', kind='t10k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = allX\n",
    "X = np.concatenate((X, allX_2), axis=0)\n",
    "y = ally\n",
    "y = np.concatenate((y, ally_2), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, trainSize=0.8, testSize=0.2, random_state=42):\n",
    "    validSize = 1 - trainSize - testSize\n",
    "    indices = np.arange(df.shape[0])\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    trainData = df.iloc[indices[:int(\n",
    "        trainSize*df.shape[0])]].reset_index(drop=True)\n",
    "    validData = df.iloc[indices[int(\n",
    "        trainSize*df.shape[0]):int((trainSize+validSize)*df.shape[0])]].reset_index(drop=True)\n",
    "    testData = df.iloc[indices[int(\n",
    "        (trainSize+validSize)*df.shape[0]):]].reset_index(drop=True)\n",
    "    if validSize == 0:\n",
    "        return trainData, testData\n",
    "    else:\n",
    "        return trainData, validData, testData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   779  780  781  782  783  0    \n",
       "0    0    0    0    0    0    5  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    4  \n",
       "3    0    0    0    0    0    1  \n",
       "4    0    0    0    0    0    9  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)\n",
    "allData = pd.concat([X, y], axis=1)\n",
    "print(allData.shape)\n",
    "allData.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData = train_test_split(\n",
    "    allData, trainSize=0.7, testSize=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainData.iloc[:, :-1]\n",
    "trainY = trainData.iloc[:, -1]\n",
    "validX = validData.iloc[:, :-1]\n",
    "validY = validData.iloc[:, -1]\n",
    "testX = testData.iloc[:, :-1]\n",
    "testY = testData.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from mlxtend.data import loadlocal_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self):\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        self.dw = []\n",
    "        self.db = []\n",
    "        self.A = []\n",
    "        self.Z = []\n",
    "        self.shape = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork():\n",
    "    \"\"\"\n",
    "    My implementation of a Neural Network Classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    acti_fns = ['relu', 'sigmoid', 'linear', 'tanh', 'softmax']\n",
    "    weight_inits = ['zero', 'random', 'normal']\n",
    "\n",
    "    def __init__(self, n_layers, layer_sizes, activation, learning_rate, weight_init, batch_size, num_epochs):\n",
    "        \"\"\"\n",
    "        Initializing a new MyNeuralNetwork object\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers : int value specifying the number of layers\n",
    "        layer_sizes : integer array of size n_layers specifying the number of nodes in each layer\n",
    "        activation : string specifying the activation function to be used\n",
    "                     possible inputs: relu, sigmoid, linear, tanh\n",
    "        learning_rate : float value specifying the learning rate to be used\n",
    "        weight_init : string specifying the weight initialization function to be used\n",
    "                      possible inputs: zero, random, normal\n",
    "        batch_size : int value specifying the batch size to be used\n",
    "        num_epochs : int value specifying the number of epochs to be used\n",
    "        \"\"\"\n",
    "        if activation not in self.acti_fns:\n",
    "            raise Exception('Incorrect Activation Function')\n",
    "\n",
    "        if weight_init not in self.weight_inits:\n",
    "            raise Exception('Incorrect Weight Initialization Function')\n",
    "        pass\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_init = weight_init\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.activation = activation\n",
    "        self.layers = self.initialize_network()\n",
    "        start_l = Layer()\n",
    "        self.layers.insert(0, start_l)  # Dummy layer\n",
    "        self.Avgcost_epoch = []\n",
    "        self.data_processing()\n",
    "\n",
    "    def data_processing(self):\n",
    "        global trainX, trainY, validX, validY, testX, testY\n",
    "        train_x, train_y = loadlocal_mnist(\n",
    "            \"Weights/Ques2/extracted/train-images-idx3-ubyte\", \"Weights/Ques2/extracted/train-labels-idx1-ubyte\")\n",
    "        test_x, test_y = loadlocal_mnist(\n",
    "            \"Weights/Ques2/extracted/t10k-images-idx3-ubyte\", \"Weights/Ques2/extracted/t10k-labels-idx1-ubyte\")\n",
    "        # train_x, train_y = loadlocal_mnist(\n",
    "            # \"/content/drive/MyDrive/ass3_data/train-images.idx3-ubyte\", \"/content/drive/MyDrive/ass3_data/train-labels.idx1-ubyte\")\n",
    "        # test_x, test_y = loadlocal_mnist(\"/content/drive/MyDrive/ass3_data/t10k-images.idx3-ubyte\",\n",
    "                                        #  \"/content/drive/MyDrive/ass3_data/t10k-labels.idx1-ubyte\")\n",
    "        # self.train_x = preprocessing.normalize(train_x)\n",
    "        # self.test_x = preprocessing.normalize(test_x)\n",
    "\n",
    "        trainMean = np.mean(train_x, axis=0)\n",
    "        trainStd = np.std(train_x, axis=0)\n",
    "        testMean = np.mean(test_x, axis=0)\n",
    "        testStd = np.std(test_x, axis=0)\n",
    "        self.train_x = (train_x - trainMean) / trainStd\n",
    "        self.test_x = (test_x - testMean) / testStd\n",
    "        enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "        self.train_y = enc.fit_transform(train_y.reshape(len(train_y), -1))\n",
    "        self.test_y = enc.transform(test_y.reshape(len(test_y), -1))\n",
    "\n",
    "    def initialize_network(self):\n",
    "        network = []\n",
    "        for i in range(1, self.n_layers):\n",
    "            network.append(self.initialize_layer(\n",
    "                self.layer_sizes[i], self.layer_sizes[i-1], self.weight_init))\n",
    "        return network\n",
    "\n",
    "    def initialize_layer(self, n_output, n_input, initialization):\n",
    "        \"\"\"\n",
    "        single layer initializer\n",
    "        n_input : input dimension\n",
    "        initialization : possible inputs: zero, random, normal\n",
    "        n_output : output dimension\n",
    "        each layer is a dictionary with weight and bias\n",
    "        \"\"\"\n",
    "        layer = Layer()\n",
    "        shape = (n_output, n_input)\n",
    "        print(\"Layershape : \", shape)\n",
    "        if initialization == \"zero\":\n",
    "            layer.weights = self.zero_init(shape)\n",
    "        if initialization == \"random\":\n",
    "            layer.weights = self.random_init(shape)\n",
    "        if initialization == \"normal\":\n",
    "            layer.weights = self.normal_init(shape)\n",
    "        # np.random.seed(1)\n",
    "        # layer.weights=np.random.randn(shape[0], shape[1]) / np.sqrt(shape[1])\n",
    "        layer.bias = np.zeros([n_output, 1])\n",
    "        return layer\n",
    "\n",
    "    def relu(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the ReLU activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        # Z=X.T\n",
    "        r = np.maximum(0, X)\n",
    "        return r\n",
    "\n",
    "    def relu_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of ReLU activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        # Z=X.T\n",
    "        return (self.relu(X) > 0)*1\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Sigmoid activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        # Z=X.T\n",
    "        s = 1/(1+np.exp(-X))\n",
    "        return s\n",
    "\n",
    "    def sigmoid_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Sigmoid activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        # Z=X.T\n",
    "        return self.sigmoid(X) * (1 - self.sigmoid(X))\n",
    "\n",
    "    def linear(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Linear activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def linear_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Linear activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "    def tanh(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the Tanh activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        # Z=X.T\n",
    "        t = np.tanh(X)\n",
    "        return t\n",
    "\n",
    "    def tanh_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Tanh activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        # Z=X.T\n",
    "        return 1-(np.power(self.tanh(X), 2))\n",
    "\n",
    "    def softmax(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the ReLU activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        # print(\"INput Shape : \",X.shape)\n",
    "        # Z=X.T\n",
    "        expX = np.exp(X - np.max(X))\n",
    "        s = expX / (expX.sum(axis=0, keepdims=True))\n",
    "        # print(\"Output Shape : \",s.T.shape)\n",
    "        return s\n",
    "\n",
    "    def softmax_grad(self, X):\n",
    "        \"\"\"\n",
    "        Calculating the gradient of Softmax activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 1-dimentional numpy array \n",
    "        Returns\n",
    "        -------\n",
    "        x_calc : 1-dimensional numpy array after calculating the necessary function over X\n",
    "        \"\"\"\n",
    "        s = X.reshape(-1, 1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    def zero_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Zero Activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "        Returns\n",
    "        -------\n",
    "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        weights = np.zeros(shape[0], shape[1])\n",
    "        return weights\n",
    "\n",
    "    def random_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Random Activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "        Returns\n",
    "        -------\n",
    "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        weights = np.random.randn(shape[0], shape[1])*0.01\n",
    "        return weights\n",
    "\n",
    "    def normal_init(self, shape):\n",
    "        \"\"\"\n",
    "        Calculating the initial weights after Normal(0,1) Activation for a particular layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : tuple specifying the shape of the layer for which weights have to be generated \n",
    "        Returns\n",
    "        -------\n",
    "        weight : 2-dimensional numpy array which contains the initial weights for the requested layer\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "        weights = 0.01*np.random.normal(size=[shape[0], shape[1]])  # change\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def transfer(self, X):\n",
    "        \"\"\"\n",
    "        Transfer function application\n",
    "        \"\"\"\n",
    "        activation = self.activation\n",
    "        if activation == \"sigmoid\":\n",
    "            return self.sigmoid(X)\n",
    "        elif activation == \"relu\":\n",
    "            return self.relu(X)\n",
    "        elif activation == \"linear\":\n",
    "            return self.linear(X)\n",
    "        elif activation == \"tanh\":\n",
    "            return self.tanh(X)\n",
    "        elif activation == \"softmax\":\n",
    "            return self.softmax(X)\n",
    "\n",
    "    def lastLayer_Softmax(self, A):\n",
    "        Z = np.dot(self.layers[-1].weights, A)+self.layers[-1].bias\n",
    "        A = self.softmax(Z)\n",
    "        self.layers[-1].A = A\n",
    "        self.layers[-1].Z = Z\n",
    "        return A\n",
    "\n",
    "    def forward_prop(self, input_r):\n",
    "        \"\"\"\n",
    "        input_r : input vector\n",
    "        Forward propogation which:-\n",
    "        --> calculates activation output\n",
    "        -->applies transfer function to it\n",
    "        -->makes output for current layer as input for next layer\n",
    "        \"\"\"\n",
    "        A = input_r.T\n",
    "        for i in range(1, len(self.layers)-1):\n",
    "            Z = np.dot(self.layers[i].weights, A)+self.layers[i].bias\n",
    "            A = self.transfer(Z)\n",
    "            self.layers[i].A = A\n",
    "            self.layers[i].Z = Z\n",
    "        A = self.lastLayer_Softmax(A)\n",
    "        return A\n",
    "\n",
    "    def transfer_grad(self, Z, dAPrev):\n",
    "        activation = self.activation\n",
    "        if activation == \"sigmoid\":\n",
    "            dZ = dAPrev * self.sigmoid_grad(Z)\n",
    "        elif activation == \"relu\":\n",
    "            dZ = dAPrev * self.relu_grad(Z)\n",
    "        elif activation == \"linear\":\n",
    "            dZ = dAPrev * self.linear_grad(Z)\n",
    "        elif activation == \"tanh\":\n",
    "            dZ = dAPrev * self.tanh_grad(Z)\n",
    "        else:\n",
    "            dZ = dAPrev * self.softmax_grad(Z)\n",
    "        return dZ\n",
    "\n",
    "    def lastLayer_backProp(self, A, Y, n):\n",
    "        dZ = A - Y.T\n",
    "        dW = np.dot(dZ, self.layers[-2].A.T) / n\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / n\n",
    "        # for last layer\n",
    "        dAPrev = self.layers[-1].weights.T.dot(dZ)\n",
    "        self.layers[-1].dw = dW\n",
    "        self.layers[-1].db = db\n",
    "        return dZ, dW, db, dAPrev\n",
    "\n",
    "    def backward_prop(self, X, Y):\n",
    "        \"\"\"\n",
    "        backward pass \n",
    "          X : input data\n",
    "          Y : true values\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        self.layers[0].A = X.T\n",
    "        A = self.layers[-1].A  # PREDICTED VALUES\n",
    "        dZ, dW, db, dAPrev = self.lastLayer_backProp(A, Y, n)\n",
    "        for layer_no in range(len(self.layers)-2, 0, -1):\n",
    "            dZ = self.transfer_grad(self.layers[layer_no].Z, dAPrev)\n",
    "            dW = 1. / n * np.dot(dZ, self.layers[layer_no-1].A.T)\n",
    "            db = 1. / n * np.sum(dZ, axis=1, keepdims=True)\n",
    "            if layer_no > 1:\n",
    "                dAPrev = np.dot(self.layers[layer_no].weights.T, dZ)\n",
    "            self.layers[layer_no].dw = dW\n",
    "            self.layers[layer_no].db = db\n",
    "\n",
    "    def update_weights(self):\n",
    "        \"\"\"\n",
    "        Used for updating weights after a forward and a backward pass\n",
    "        \"\"\"\n",
    "        for l in range(1, len(self.layers)):\n",
    "            self.layers[l].weights = self.layers[l].weights - \\\n",
    "                ((self.learning_rate)*self.layers[l].dw)\n",
    "            self.layers[l].bias = self.layers[l].bias - \\\n",
    "                ((self.learning_rate)*self.layers[l].db)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Fitting (training) the linear model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as training data.\n",
    "        Y : 1-dimensional numpy array of shape (n_samples,) which acts as training labels.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : an instance of self\n",
    "        \"\"\"\n",
    "        X = self.train_x\n",
    "        Y = self.train_y\n",
    "        self.test_epoch_error = []\n",
    "        for epoch in range(self.num_epochs):\n",
    "            X, Y = shuffle(X, Y, random_state=1)\n",
    "            num_batches = int(X.shape[0]/self.batch_size)\n",
    "            train_x = np.vsplit(X, num_batches)\n",
    "            train_y = np.vsplit(Y, num_batches)\n",
    "            cost, cost_batch = self.fit_batch(train_x, train_y, num_batches)\n",
    "            avg_epoch_cost = cost/len(cost_batch)\n",
    "            self.Avgcost_epoch.append(avg_epoch_cost)\n",
    "\n",
    "            A_test = self.forward_prop(self.test_x)\n",
    "            self.test_epoch_error.append(self.cost(self.test_y, A_test))\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Epoch no.\", epoch, \" Cost:\", avg_epoch_cost,\n",
    "                      \"Accuracy:\", self.score(X, Y))\n",
    "        # fit function has to return an instance of itself or else it won't work with test.py\n",
    "        return self\n",
    "\n",
    "    def fit_batch(self, train_x, train_y, num_batches):\n",
    "        \"\"\"\n",
    "        train_x : splitted array of X train set\n",
    "        train_y : splitted array of Y train set\n",
    "        num_batches : number of Batches\n",
    "        cost : returns the total cost for all batches\n",
    "        cost_batch : returns arr of all costs of batches\n",
    "        \"\"\"\n",
    "        cost = 0\n",
    "        cost_batch = []\n",
    "        for i in range(num_batches):\n",
    "            A = self.forward_prop(train_x[i])\n",
    "            c = self.cost(train_y[i], A)\n",
    "            cost += c\n",
    "            cost_batch.append(c)\n",
    "            self.backward_prop(train_x[i], train_y[i])\n",
    "            self.update_weights()\n",
    "        return cost, cost_batch\n",
    "\n",
    "    def cost(self, Y, A):\n",
    "        return -np.mean(Y * np.log(A.T + 1e-8))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predicting probabilities using the trained linear model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "        Returns\n",
    "        -------\n",
    "        y : 2-dimensional numpy array of shape (n_samples, n_classes) which contains the \n",
    "            class wise prediction probabilities.\n",
    "        \"\"\"\n",
    "        A = self.forward_prop(X)\n",
    "        # return the numpy array y which contains the predicted values\n",
    "        return A\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicting values using the trained linear model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "        Returns\n",
    "        -------\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which contains the predicted values.\n",
    "        \"\"\"\n",
    "        A = self.predict_proba(X)\n",
    "        y_pred = np.argmax(A, axis=0).T\n",
    "        # return the numpy array y which contains the predicted values\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        \"\"\"\n",
    "        Predicting values using the trained linear model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2-dimensional numpy array of shape (n_samples, n_features) which acts as testing data.\n",
    "        y : 1-dimensional numpy array of shape (n_samples,) which acts as testing labels.\n",
    "        Returns\n",
    "        -------\n",
    "        acc : float value specifying the accuracy of the model on the provided testing set\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        acc = np.mean(y_pred == Y)*100\n",
    "        # return the numpy array y which contains the predicted values\n",
    "        return acc\n",
    "\n",
    "    def saveModel(self, model, filename):\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "    def loadModel(self, filename):\n",
    "        loaded_model = pickle.load(open(filename, 'rb'))\n",
    "        return loaded_model\n",
    "\n",
    "    def svd(self, X):\n",
    "        svd = TruncatedSVD(n_components=20, random_state=0)\n",
    "        X_svd = svd.fit(X).transform(X)\n",
    "        return X_svd\n",
    "\n",
    "    def tsne(self):\n",
    "        # TSNE\n",
    "        A = self.forward_prop(self.test_x)\n",
    "        Y = np.argmax(self.test_y, axis=1)\n",
    "        plot_mat = self.layers[-2].A.T\n",
    "        X_svd = self.svd(plot_mat)\n",
    "        tsn = TSNE(random_state=0, n_components=2)\n",
    "        X_sn = tsn.fit_transform(X_svd)\n",
    "        sns.scatterplot(X_sn[:, 0], X_sn[:, 1], hue=Y,\n",
    "                        legend='full', palette=sns.color_palette(\"hls\", 10))\n",
    "        plt.savefig(\"TSNE\")\n",
    "        plt.show()\n",
    "\n",
    "    def SkLearn(self, activationn, batch_size=200):\n",
    "        clf = MLPClassifier(activation=activationn, max_iter=100, learning_rate_init=0.1, random_state=1,\n",
    "                            hidden_layer_sizes=(256, 128, 64), batch_size=batch_size).fit(self.train_x, self.train_y)\n",
    "        print(activationn, \"Score\", clf.score(self.test_x, self.test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layershape :  (256, 784)\n",
      "Layershape :  (128, 256)\n",
      "Layershape :  (64, 128)\n",
      "Layershape :  (32, 64)\n",
      "Layershape :  (10, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_4835/2894383915.py:61: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.train_x = (train_x - trainMean) / trainStd\n",
      "/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_4835/2894383915.py:62: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.test_x = (test_x - testMean) / testStd\n"
     ]
    }
   ],
   "source": [
    "network = MyNeuralNetwork(n_layers=6, layer_sizes=[\n",
    "                             784, 256, 128, 64, 32, 10], activation=\"relu\", learning_rate=0.1, weight_init=\"normal\", batch_size=200, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no. 0  Cost: nan Accuracy: 9.922448979591838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MyNeuralNetwork at 0x7f89bd0492e0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(network.train_x, network.train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = load_mnist('Weights/Ques2/', kind='train')\n",
    "testX, testY = load_mnist('Weights/Ques2/', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "        126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "        253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "        253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "        253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "        253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "        183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "        229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "        221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "        213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "        219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "        226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = trainX[:1]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAutoNormalize = preprocessing.normalize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00123091, 0.00738549, 0.00738549,\n",
       "        0.00738549, 0.0516984 , 0.05580145, 0.07180334, 0.01066792,\n",
       "        0.0681106 , 0.10462772, 0.10134528, 0.05210871, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01230914, 0.01477097, 0.03856865, 0.06318694,\n",
       "        0.06975182, 0.10380711, 0.10380711, 0.10380711, 0.10380711,\n",
       "        0.10380711, 0.09231858, 0.07057243, 0.10380711, 0.09929376,\n",
       "        0.08000944, 0.02625951, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02010494, 0.09765254,\n",
       "        0.10380711, 0.10380711, 0.10380711, 0.10380711, 0.10380711,\n",
       "        0.10380711, 0.10380711, 0.10380711, 0.1029865 , 0.03815835,\n",
       "        0.03364499, 0.03364499, 0.02297707, 0.01600189, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00738549, 0.08985675, 0.10380711, 0.10380711,\n",
       "        0.10380711, 0.10380711, 0.10380711, 0.08124035, 0.07467547,\n",
       "        0.10134528, 0.09888346, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.03282438, 0.06400755, 0.04390261, 0.10380711, 0.10380711,\n",
       "        0.08411248, 0.00451335, 0.        , 0.01764311, 0.06318694,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00574427,\n",
       "        0.0004103 , 0.06318694, 0.10380711, 0.03692743, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05703237,\n",
       "        0.10380711, 0.07795791, 0.00082061, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00451335, 0.07795791, 0.10380711,\n",
       "        0.02872134, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01436067, 0.09888346, 0.09231858, 0.06564877,\n",
       "        0.04431292, 0.0004103 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.03323469, 0.09847315, 0.10380711, 0.10380711, 0.04882627,\n",
       "        0.01025762, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01846372,\n",
       "        0.07631669, 0.10380711, 0.10380711, 0.06154572, 0.01107823,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00656488, 0.03815835,\n",
       "        0.10339681, 0.10380711, 0.076727  , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.10216589, 0.10380711,\n",
       "        0.10216589, 0.02625951, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.01887402, 0.05333962,\n",
       "        0.07508578, 0.10380711, 0.10380711, 0.08493309, 0.00082061,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01600189,\n",
       "        0.06072511, 0.0939598 , 0.10380711, 0.10380711, 0.10380711,\n",
       "        0.1025762 , 0.07467547, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00984732, 0.04677475, 0.09067736, 0.10380711, 0.10380711,\n",
       "        0.10380711, 0.10380711, 0.08247126, 0.03200377, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00943701, 0.02708012, 0.08739492, 0.10380711,\n",
       "        0.10380711, 0.10380711, 0.10380711, 0.08124035, 0.03323469,\n",
       "        0.00082061, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00738549, 0.07016212, 0.08985675,\n",
       "        0.10380711, 0.10380711, 0.10380711, 0.10380711, 0.08000944,\n",
       "        0.03282438, 0.00369274, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02256676, 0.07057243,\n",
       "        0.09272888, 0.10380711, 0.10380711, 0.10380711, 0.10380711,\n",
       "        0.10011437, 0.05457054, 0.00451335, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.05580145, 0.10380711, 0.10380711, 0.10380711,\n",
       "        0.08698462, 0.05539115, 0.05416023, 0.00656488, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAutoNormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2437.212547152997"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLinNorm = np.linalg.norm(test)\n",
    "testLinNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00123091, 0.00738549, 0.00738549,\n",
       "        0.00738549, 0.0516984 , 0.05580145, 0.07180334, 0.01066792,\n",
       "        0.0681106 , 0.10462772, 0.10134528, 0.05210871, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01230914, 0.01477097, 0.03856865, 0.06318694,\n",
       "        0.06975182, 0.10380711, 0.10380711, 0.10380711, 0.10380711,\n",
       "        0.10380711, 0.09231858, 0.07057243, 0.10380711, 0.09929376,\n",
       "        0.08000944, 0.02625951, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02010494, 0.09765254,\n",
       "        0.10380711, 0.10380711, 0.10380711, 0.10380711, 0.10380711,\n",
       "        0.10380711, 0.10380711, 0.10380711, 0.1029865 , 0.03815835,\n",
       "        0.03364499, 0.03364499, 0.02297707, 0.01600189, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00738549, 0.08985675, 0.10380711, 0.10380711,\n",
       "        0.10380711, 0.10380711, 0.10380711, 0.08124035, 0.07467547,\n",
       "        0.10134528, 0.09888346, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.03282438, 0.06400755, 0.04390261, 0.10380711, 0.10380711,\n",
       "        0.08411248, 0.00451335, 0.        , 0.01764311, 0.06318694,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00574427,\n",
       "        0.0004103 , 0.06318694, 0.10380711, 0.03692743, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05703237,\n",
       "        0.10380711, 0.07795791, 0.00082061, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00451335, 0.07795791, 0.10380711,\n",
       "        0.02872134, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01436067, 0.09888346, 0.09231858, 0.06564877,\n",
       "        0.04431292, 0.0004103 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.03323469, 0.09847315, 0.10380711, 0.10380711, 0.04882627,\n",
       "        0.01025762, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01846372,\n",
       "        0.07631669, 0.10380711, 0.10380711, 0.06154572, 0.01107823,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00656488, 0.03815835,\n",
       "        0.10339681, 0.10380711, 0.076727  , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.10216589, 0.10380711,\n",
       "        0.10216589, 0.02625951, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.01887402, 0.05333962,\n",
       "        0.07508578, 0.10380711, 0.10380711, 0.08493309, 0.00082061,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01600189,\n",
       "        0.06072511, 0.0939598 , 0.10380711, 0.10380711, 0.10380711,\n",
       "        0.1025762 , 0.07467547, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00984732, 0.04677475, 0.09067736, 0.10380711, 0.10380711,\n",
       "        0.10380711, 0.10380711, 0.08247126, 0.03200377, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00943701, 0.02708012, 0.08739492, 0.10380711,\n",
       "        0.10380711, 0.10380711, 0.10380711, 0.08124035, 0.03323469,\n",
       "        0.00082061, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00738549, 0.07016212, 0.08985675,\n",
       "        0.10380711, 0.10380711, 0.10380711, 0.10380711, 0.08000944,\n",
       "        0.03282438, 0.00369274, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02256676, 0.07057243,\n",
       "        0.09272888, 0.10380711, 0.10380711, 0.10380711, 0.10380711,\n",
       "        0.10011437, 0.05457054, 0.00451335, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.05580145, 0.10380711, 0.10380711, 0.10380711,\n",
       "        0.08698462, 0.05539115, 0.05416023, 0.00656488, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLinNormalized = test / testLinNorm\n",
    "testLinNormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLinNormalized == testAutoNormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
