{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allX, ally = load_mnist('Weights/Ques2/', kind='train')\n",
    "allX_2, ally_2 = load_mnist('Weights/Ques2/', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = allX\n",
    "X = np.concatenate((X, allX_2), axis=0)\n",
    "y = ally\n",
    "y = np.concatenate((y, ally_2), axis=0)\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Weights/Ques2/extracted/t10k-images-idx3-ubyte', 'rb') as f:\n",
    "    data = f.read()\n",
    "    test_images = np.frombuffer(data, dtype=np.uint8, offset=16).reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, trainSize=0.8, testSize=0.2, random_state=42):\n",
    "    validSize = 1 - trainSize - testSize\n",
    "    indices = np.arange(df.shape[0])\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    trainData = df.iloc[indices[:int(\n",
    "        trainSize*df.shape[0])]].reset_index(drop=True)\n",
    "    validData = df.iloc[indices[int(\n",
    "        trainSize*df.shape[0]):int((trainSize+validSize)*df.shape[0])]].reset_index(drop=True)\n",
    "    testData = df.iloc[indices[int(\n",
    "        (trainSize+validSize)*df.shape[0]):]].reset_index(drop=True)\n",
    "    if validSize == 0:\n",
    "        return trainData, testData\n",
    "    else:\n",
    "        return trainData, validData, testData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   779  780  781  782  783  0    \n",
       "0    0    0    0    0    0    5  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    4  \n",
       "3    0    0    0    0    0    1  \n",
       "4    0    0    0    0    0    9  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allData = pd.concat([X, y], axis=1)\n",
    "print(allData.shape)\n",
    "allData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData = train_test_split(allData, trainSize=0.7, testSize=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49000, 785), (7000, 785), (14000, 785))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.shape, validData.shape, testData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainData.iloc[:, :-1]\n",
    "trainY = trainData.iloc[:, -1]\n",
    "validX = validData.iloc[:, :-1]\n",
    "validY = validData.iloc[:, -1]\n",
    "testX = testData.iloc[:, :-1]\n",
    "testY = testData.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork:\n",
    "    def __init__(self, N_inputs, N_outputs, N_layers=2, Layer_sizes=[10, 5], activation=\"sigmoid\", learning_rate=0.1, weight_init=\"random\", batch_size=1, num_epochs=200):\n",
    "        \"\"\"\n",
    "        N_inputs: input size\n",
    "        N_outputs: outputs size\n",
    "        N_layers: number of hidden layers\n",
    "        Layer_sizes: list of hidden layer sizes\n",
    "        activation: activation function to be used (ReLu, Leaky ReLu, sigmoid, linear, tanh, softmax)\n",
    "        learning_rate: learning rate\n",
    "        weight_init: weight initialization (zero, random, normal)\n",
    "        batch_size: batch size\n",
    "        num_epochs: number of epochs\n",
    "        \"\"\"\n",
    "        self.N_inputs = N_inputs\n",
    "        self.N_outputs = N_outputs\n",
    "        self.N_layers = N_layers\n",
    "        self.Layer_sizes = Layer_sizes\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_init = weight_init\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        np.random.seed(0)\n",
    "\n",
    "        model = {}\n",
    "        if weight_init == \"zero\":\n",
    "            model['W1'] = np.zeros((N_inputs, Layer_sizes[0]))\n",
    "            model['b1'] = np.zeros((1, Layer_sizes[0]))\n",
    "            for i in range(1, N_layers):\n",
    "                model['W' + str(i+1)] = np.zeros((Layer_sizes[i-1], Layer_sizes[i]))\n",
    "                model['b' + str(i+1)] = np.zeros((1, Layer_sizes[i]))\n",
    "            model['W' + str(N_layers+1)] = np.zeros((Layer_sizes[-1], N_outputs))\n",
    "            model['b' + str(N_layers+1)] = np.zeros((1, N_outputs))\n",
    "        elif weight_init == \"random\":\n",
    "            model['W1'] = np.random.randn(N_inputs, Layer_sizes[0])\n",
    "            model['W1'] *= 0.01\n",
    "            # model['W1'] = np.random.randn(N_inputs, Layer_sizes[0])\n",
    "            model['b1'] = np.zeros((1, Layer_sizes[0]))\n",
    "            for i in range(1, N_layers):\n",
    "                model['W' + str(i+1)] = np.random.randn(Layer_sizes[i-1], Layer_sizes[i])\n",
    "                model['W' + str(i+1)] *= 0.00000001\n",
    "                # model['W' + str(i+1)] = np.random.randn(Layer_sizes[i-1], Layer_sizes[i])\n",
    "                model['b' + str(i+1)] = np.zeros((1, Layer_sizes[i]))\n",
    "            model['W' + str(N_layers+1)] = np.random.randn(Layer_sizes[-1], N_outputs)\n",
    "            model['W' + str(N_layers+1)] *= 0.00000001\n",
    "            # model['W' + str(N_layers+1)] = np.random.randn(Layer_sizes[-1], N_outputs)\n",
    "            model['b' + str(N_layers+1)] = np.zeros((1, N_outputs))\n",
    "        elif weight_init == \"normal\":\n",
    "            model['W1'] = np.random.normal(0, 1, (N_inputs, Layer_sizes[0]))*0.00000001\n",
    "            model['b1'] = np.zeros((1, Layer_sizes[0]))\n",
    "            for i in range(1, N_layers):\n",
    "                model['W' + str(i+1)] = np.random.normal(0, 1, (Layer_sizes[i-1], Layer_sizes[i]))*0.01\n",
    "                model['b' + str(i+1)] = np.zeros((1, Layer_sizes[i]))\n",
    "            model['W' + str(N_layers+1)] = np.random.normal(0, 1, (Layer_sizes[-1], N_outputs))*0.01\n",
    "            model['b' + str(N_layers+1)] = np.zeros((1, N_outputs))\n",
    "        else:\n",
    "            print(\"Invalid weight initialization\")\n",
    "            return\n",
    "\n",
    "        self.model = model\n",
    "        self.activationOutputs = None\n",
    "    \n",
    "    def relu_forward(self, X):\n",
    "        \"\"\"\n",
    "        ReLu activation function for forward propagation\n",
    "        X: input\n",
    "        return: output after applying the relu function\n",
    "        \"\"\"\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "    def relu_backward(self, X):\n",
    "        \"\"\"\n",
    "        ReLu activation function for backpropagation\n",
    "        X: input\n",
    "        return: output after applying the gradient of relu function\n",
    "        \"\"\"\n",
    "        return np.where(X > 0, 1, 0)\n",
    "\n",
    "    def leaky_relu_forward(self, X):\n",
    "        \"\"\"\n",
    "        Leaky ReLu activation function\n",
    "        X: input\n",
    "        return: output after applying the Leaky ReLu function\n",
    "        \"\"\"\n",
    "        return np.maximum(X, 0.01*X)\n",
    "\n",
    "    def leaky_relu_backward(self, X):\n",
    "        \"\"\"\n",
    "        Leaky ReLu activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of Leaky ReLu function\n",
    "        \"\"\"\n",
    "        return np.where(X > 0, 1, 0.01)\n",
    "\n",
    "    def sigmoid_forward(self, X):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function\n",
    "        X: input\n",
    "        return: output after applying the sigmoid function\n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-X))\n",
    "\n",
    "    def sigmoid_backward(self, X):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of sigmoid function\n",
    "        \"\"\"\n",
    "        return self.sigmoid_forward(X)*(1-self.sigmoid_forward(X))\n",
    "        # return X*(1-X)\n",
    "\n",
    "    def linear_forward(self, X):\n",
    "        \"\"\"\n",
    "        Linear activation function\n",
    "        X: input\n",
    "        return: output after applying the linear function\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def linear_backward(self, X):\n",
    "        \"\"\"\n",
    "        Linear activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of linear function\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "    def tanh_forward(self, X):\n",
    "        \"\"\"\n",
    "        Tanh activation function\n",
    "        X: input\n",
    "        return: output after applying the tanh function\n",
    "        \"\"\"\n",
    "        return (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
    "    \n",
    "    def tanh_backward(self, X):\n",
    "        \"\"\"\n",
    "        Tanh activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of tanh function\n",
    "        \"\"\"\n",
    "        return 1-self.tanh_forward(X)**2\n",
    "        # return 1-X**2\n",
    "\n",
    "    def softmax_forward(self, X):\n",
    "        \"\"\"\n",
    "        Softmax activation function\n",
    "        X: input\n",
    "        return: output after applying the softmax function\n",
    "        \"\"\"\n",
    "        return np.exp(X)/np.sum(np.exp(X), axis=1, keepdims=True)\n",
    "\n",
    "    # softmax implemetatation incomplete\n",
    "    def softmax_backward(self, X):\n",
    "        \"\"\"\n",
    "        Softmax activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of softmax function\n",
    "        \"\"\"\n",
    "        softmax = self.softmax_forward(X)\n",
    "        e = np.ones((softmax.shape[0], 1))\n",
    "        v1 = np.dot(softmax, e.T)\n",
    "        i = np.eye(softmax.shape[0])\n",
    "        v2 = i - np.dot(e, softmax.T)\n",
    "        return np.multiply(v1, v2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        X: input\n",
    "        return: output after applying the activation function\n",
    "        \"\"\"\n",
    "        if self.activation == \"relu\":\n",
    "            currentActivationFuntion = self.relu_forward\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            currentActivationFuntion = self.leaky_relu_forward\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            currentActivationFuntion = self.sigmoid_forward\n",
    "        elif self.activation == \"linear\":\n",
    "            currentActivationFuntion = self.linear_forward\n",
    "        elif self.activation == \"tanh\":\n",
    "            currentActivationFuntion = self.tanh_forward\n",
    "        elif self.activation == \"softmax\":\n",
    "            currentActivationFuntion = self.softmax_forward\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "        \n",
    "        self.activationOutputs = {}\n",
    "        \n",
    "        self.activationOutputs['Z1'] = np.dot(X, self.model['W1']) + self.model['b1']\n",
    "        self.activationOutputs['A1'] = currentActivationFuntion(self.activationOutputs['Z1'])\n",
    "        # self.activationOutputs['A1'] = np.tanh(self.activationOutputs['Z1'])\n",
    "\n",
    "        for i in range(2, self.N_layers+1):\n",
    "            self.activationOutputs['Z' + str(i)] = np.dot(self.activationOutputs['A' + str(i-1)], self.model['W' + str(i)]) + self.model['b' + str(i)]\n",
    "            self.activationOutputs['A' + str(i)] = currentActivationFuntion(self.activationOutputs['Z' + str(i)])\n",
    "\n",
    "        self.activationOutputs['Z' + str(self.N_layers+1)] = np.dot(self.activationOutputs['A' + str(self.N_layers)], self.model['W' + str(self.N_layers+1)]) + self.model['b' + str(self.N_layers+1)]\n",
    "        self.activationOutputs['A' + str(self.N_layers+1)] = currentActivationFuntion(self.activationOutputs['Z' + str(self.N_layers+1)])\n",
    "\n",
    "        return self.activationOutputs['A' + str(self.N_layers+1)]\n",
    "\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Backward propagation\n",
    "        X: input\n",
    "        Y: output\n",
    "        \"\"\"\n",
    "        if self.activation == \"relu\":\n",
    "            currentActivationFuntion = self.relu_backward\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            currentActivationFuntion = self.leaky_relu_backward\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            currentActivationFuntion = self.sigmoid_backward\n",
    "        elif self.activation == \"linear\":\n",
    "            currentActivationFuntion = self.linear_backward\n",
    "        elif self.activation == \"tanh\":\n",
    "            currentActivationFuntion = self.tanh_backward\n",
    "        elif self.activation == \"softmax\":\n",
    "            currentActivationFuntion = self.softmax_backward\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "        # computing the gradients\n",
    "        self.gradients = {}\n",
    "        self.gradients['delta' + str(self.N_layers+1)] = self.activationOutputs['A' + str(self.N_layers+1)] - Y\n",
    "        self.gradients['dW' + str(self.N_layers+1)] = np.dot(self.activationOutputs['A' + str(self.N_layers)].T, self.gradients['delta' + str(self.N_layers+1)])\n",
    "        self.gradients['db' + str(self.N_layers+1)] = np.sum(self.gradients['delta' + str(self.N_layers+1)], axis=0)\n",
    "        \n",
    "        for i in range(self.N_layers, 1, -1):\n",
    "            self.gradients['delta' + str(i)] = np.dot(self.gradients['delta' + str(i+1)], self.model['W' + str(i+1)].T) * currentActivationFuntion(self.activationOutputs['Z' + str(i)])\n",
    "            self.gradients['dW' + str(i)] = np.dot(self.activationOutputs['A' + str(i-1)].T, self.gradients['delta' + str(i)])\n",
    "            self.gradients['db' + str(i)] = np.sum(self.gradients['delta' + str(i)], axis=0)\n",
    "\n",
    "        self.gradients['delta1'] = np.dot(self.gradients['delta2'], self.model['W2'].T) * currentActivationFuntion(self.activationOutputs['Z1'])\n",
    "        self.gradients['dW1'] = np.dot(X.T, self.gradients['delta1'])\n",
    "        self.gradients['db1'] = np.sum(self.gradients['delta1'], axis=0)\n",
    "\n",
    "        # updating the model parameters\n",
    "        for i in range(1, self.N_layers+2):\n",
    "            self.model['W' + str(i)] -= self.learning_rate * self.gradients['dW' + str(i)]\n",
    "            self.model['b' + str(i)] -= self.learning_rate * self.gradients['db' + str(i)]\n",
    "\n",
    "    def oneHotEncoder(self, y, n_classes):\n",
    "        \"\"\"\n",
    "        One hot encoder\n",
    "        y: input\n",
    "        return: encoded output\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        y_oht = np.zeros((m, n_classes))\n",
    "        y_oht[np.arange(m), y] = 1\n",
    "        return y_oht\n",
    "\n",
    "    def crossEntropyLoss(self, y_oht, y_prob):\n",
    "        \"\"\"\n",
    "        Cross entropy loss\n",
    "        y_oht: one hot encoded output\n",
    "        y_prob: probabilities for classes\n",
    "        return: cross entropy loss\n",
    "        \"\"\"\n",
    "        return -np.mean(y_oht * np.log(y_prob))\n",
    "\n",
    "    def fit(self, X, y, logs=True):\n",
    "        \"\"\"\n",
    "        Fit the model to the data\n",
    "        X: input\n",
    "        Y: output\n",
    "        epochs: number of epochs\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        classes = self.N_outputs\n",
    "        batchSize = self.batch_size\n",
    "        y_oht = self.oneHotEncoder(y, classes)\n",
    "        for i in range(self.num_epochs):\n",
    "            for j in range(0, X.shape[0], batchSize):\n",
    "                X_batch = X[j:j+batchSize]\n",
    "                y_batch = y_oht[j:j+batchSize]\n",
    "                y_ = self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch)\n",
    "            y_ = self.forward(X)\n",
    "            loss = self.crossEntropyLoss(y_oht, y_)\n",
    "            losses.append(loss)\n",
    "            if logs:\n",
    "                print(\"Epoch: {}, Loss: {}\".format(i, loss))\n",
    "        return losses\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probabilities\n",
    "        X: input\n",
    "        return: probabilities\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes\n",
    "        X: input\n",
    "        return: classes\n",
    "        \"\"\"\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Score the model\n",
    "        X: input\n",
    "        Y: output\n",
    "        return: accuracy\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNeuralNetwork(N_inputs=784, N_outputs=10, N_layers=1, Layer_sizes=[100], activation=\"relu\", learning_rate=0.08, weight_init=\"random\", num_epochs=10, batch_size=trainX.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_ = trainX.values\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.values.astype(np.float128)\n",
    "trainY = trainY.values.astype(np.float128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_49070/1716970987.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlossObtained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_49070/764094008.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, logs)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mbatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0my_oht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_49070/764094008.py\u001b[0m in \u001b[0;36moneHotEncoder\u001b[0;34m(self, y, n_classes)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0my_oht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0my_oht\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_oht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "lossObtained = model.fit(trainX, trainY, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_49070/764094008.py:101: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 11.521428571428572\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(model.score(testX, testY)))\n",
    "plt.plot(lossObtained)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(verbose=True, max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=150, learning_rate_init=0.08, activation='relu', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.81702758\n",
      "Iteration 2, loss = 2.29174322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:619: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(learning_rate_init=0.08, max_iter=150, verbose=True)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(allX, ally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9710714285714286"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relu'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of MLPClassifier(max_iter=20, verbose=True)>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.n_outputs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.learning_rate_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.hidden_layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10285714285714286, 0.10497959183673469)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(validX, validY), mlp.score(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "vinayTrainX = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "vinayTrainY = [0, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNeuralNetwork(N_inputs=2, N_outputs=2, N_layers=1, Layer_sizes=[5], activation=\"sigmoid\", learning_rate=0.1, weight_init=\"random\", num_epochs=100, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 1), (4, 2))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vinayTrainY = np.array(vinayTrainY).reshape(4, 1)\n",
    "vinayTrainX = np.array(vinayTrainX).reshape(4, 2)\n",
    "vinayTrainY.shape,vinayTrainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.007630946930713027\n",
      "Epoch: 1, Loss: 0.007545724145032244\n",
      "Epoch: 2, Loss: 0.00746228427390646\n",
      "Epoch: 3, Loss: 0.007380573220953148\n",
      "Epoch: 4, Loss: 0.007300539027734195\n",
      "Epoch: 5, Loss: 0.007222131770081296\n",
      "Epoch: 6, Loss: 0.007145303460358473\n",
      "Epoch: 7, Loss: 0.007070007955271976\n",
      "Epoch: 8, Loss: 0.0069962008688648665\n",
      "Epoch: 9, Loss: 0.006923839490360749\n",
      "Epoch: 10, Loss: 0.00685288270654557\n",
      "Epoch: 11, Loss: 0.006783290928399731\n",
      "Epoch: 12, Loss: 0.006715026021711789\n",
      "Epoch: 13, Loss: 0.006648051241425602\n",
      "Epoch: 14, Loss: 0.006582331169489653\n",
      "Epoch: 15, Loss: 0.006517831655993464\n",
      "Epoch: 16, Loss: 0.006454519763390485\n",
      "Epoch: 17, Loss: 0.006392363713622457\n",
      "Epoch: 18, Loss: 0.006331332837969828\n",
      "Epoch: 19, Loss: 0.006271397529467371\n",
      "Epoch: 20, Loss: 0.00621252919773396\n",
      "Epoch: 21, Loss: 0.006154700226074842\n",
      "Epoch: 22, Loss: 0.006097883930725332\n",
      "Epoch: 23, Loss: 0.006042054522112215\n",
      "Epoch: 24, Loss: 0.005987187068018646\n",
      "Epoch: 25, Loss: 0.005933257458543711\n",
      "Epoch: 26, Loss: 0.005880242372756806\n",
      "Epoch: 27, Loss: 0.005828119246952312\n",
      "Epoch: 28, Loss: 0.00577686624441481\n",
      "Epoch: 29, Loss: 0.005726462226614194\n",
      "Epoch: 30, Loss: 0.0056768867257509575\n",
      "Epoch: 31, Loss: 0.005628119918579529\n",
      "Epoch: 32, Loss: 0.0055801426014405915\n",
      "Epoch: 33, Loss: 0.005532936166438941\n",
      "Epoch: 34, Loss: 0.005486482578704834\n",
      "Epoch: 35, Loss: 0.005440764354683724\n",
      "Epoch: 36, Loss: 0.005395764541399454\n",
      "Epoch: 37, Loss: 0.005351466696640751\n",
      "Epoch: 38, Loss: 0.005307854870024673\n",
      "Epoch: 39, Loss: 0.005264913584891192\n",
      "Epoch: 40, Loss: 0.005222627820986367\n",
      "Epoch: 41, Loss: 0.005180982997896615\n",
      "Epoch: 42, Loss: 0.005139964959194217\n",
      "Epoch: 43, Loss: 0.005099559957260188\n",
      "Epoch: 44, Loss: 0.005059754638750328\n",
      "Epoch: 45, Loss: 0.005020536030674202\n",
      "Epoch: 46, Loss: 0.00498189152705561\n",
      "Epoch: 47, Loss: 0.0049438088761478765\n",
      "Epoch: 48, Loss: 0.004906276168176602\n",
      "Epoch: 49, Loss: 0.004869281823585237\n",
      "Epoch: 50, Loss: 0.004832814581759293\n",
      "Epoch: 51, Loss: 0.0047968634902065595\n",
      "Epoch: 52, Loss: 0.004761417894173137\n",
      "Epoch: 53, Loss: 0.004726467426673171\n",
      "Epoch: 54, Loss: 0.004692001998915512\n",
      "Epoch: 55, Loss: 0.004658011791106651\n",
      "Epoch: 56, Loss: 0.004624487243615008\n",
      "Epoch: 57, Loss: 0.004591419048479126\n",
      "Epoch: 58, Loss: 0.004558798141243983\n",
      "Epoch: 59, Loss: 0.004526615693111493\n",
      "Epoch: 60, Loss: 0.004494863103391758\n",
      "Epoch: 61, Loss: 0.004463531992239766\n",
      "Epoch: 62, Loss: 0.004432614193666866\n",
      "Epoch: 63, Loss: 0.004402101748815337\n",
      "Epoch: 64, Loss: 0.004371986899482173\n",
      "Epoch: 65, Loss: 0.004342262081884165\n",
      "Epoch: 66, Loss: 0.00431291992065248\n",
      "Epoch: 67, Loss: 0.004283953223047294\n",
      "Epoch: 68, Loss: 0.004255354973382484\n",
      "Epoch: 69, Loss: 0.00422711832765358\n",
      "Epoch: 70, Loss: 0.0041992366083573734\n",
      "Epoch: 71, Loss: 0.004171703299498503\n",
      "Epoch: 72, Loss: 0.004144512041772719\n",
      "Epoch: 73, Loss: 0.004117656627920653\n",
      "Epoch: 74, Loss: 0.004091130998245622\n",
      "Epoch: 75, Loss: 0.004064929236287665\n",
      "Epoch: 76, Loss: 0.004039045564648131\n",
      "Epoch: 77, Loss: 0.004013474340958937\n",
      "Epoch: 78, Loss: 0.003988210053989949\n",
      "Epoch: 79, Loss: 0.003963247319890445\n",
      "Epoch: 80, Loss: 0.003938580878557269\n",
      "Epoch: 81, Loss: 0.00391420559012677\n",
      "Epoch: 82, Loss: 0.003890116431584271\n",
      "Epoch: 83, Loss: 0.003866308493487156\n",
      "Epoch: 84, Loss: 0.003842776976797196\n",
      "Epoch: 85, Loss: 0.0038195171898175433\n",
      "Epoch: 86, Loss: 0.0037965245452305668\n",
      "Epoch: 87, Loss: 0.003773794557232673\n",
      "Epoch: 88, Loss: 0.0037513228387628487\n",
      "Epoch: 89, Loss: 0.003729105098820751\n",
      "Epoch: 90, Loss: 0.0037071371398710194\n",
      "Epoch: 91, Loss: 0.0036854148553314464\n",
      "Epoch: 92, Loss: 0.0036639342271408736\n",
      "Epoch: 93, Loss: 0.0036426913234043168\n",
      "Epoch: 94, Loss: 0.0036216822961127157\n",
      "Epoch: 95, Loss: 0.003600903378933896\n",
      "Epoch: 96, Loss: 0.003580350885073308\n",
      "Epoch: 97, Loss: 0.003560021205200529\n",
      "Epoch: 98, Loss: 0.003539910805440503\n",
      "Epoch: 99, Loss: 0.0035200162254262857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.007630946930713027,\n",
       " 0.007545724145032244,\n",
       " 0.00746228427390646,\n",
       " 0.007380573220953148,\n",
       " 0.007300539027734195,\n",
       " 0.007222131770081296,\n",
       " 0.007145303460358473,\n",
       " 0.007070007955271976,\n",
       " 0.0069962008688648665,\n",
       " 0.006923839490360749,\n",
       " 0.00685288270654557,\n",
       " 0.006783290928399731,\n",
       " 0.006715026021711789,\n",
       " 0.006648051241425602,\n",
       " 0.006582331169489653,\n",
       " 0.006517831655993464,\n",
       " 0.006454519763390485,\n",
       " 0.006392363713622457,\n",
       " 0.006331332837969828,\n",
       " 0.006271397529467371,\n",
       " 0.00621252919773396,\n",
       " 0.006154700226074842,\n",
       " 0.006097883930725332,\n",
       " 0.006042054522112215,\n",
       " 0.005987187068018646,\n",
       " 0.005933257458543711,\n",
       " 0.005880242372756806,\n",
       " 0.005828119246952312,\n",
       " 0.00577686624441481,\n",
       " 0.005726462226614194,\n",
       " 0.0056768867257509575,\n",
       " 0.005628119918579529,\n",
       " 0.0055801426014405915,\n",
       " 0.005532936166438941,\n",
       " 0.005486482578704834,\n",
       " 0.005440764354683724,\n",
       " 0.005395764541399454,\n",
       " 0.005351466696640751,\n",
       " 0.005307854870024673,\n",
       " 0.005264913584891192,\n",
       " 0.005222627820986367,\n",
       " 0.005180982997896615,\n",
       " 0.005139964959194217,\n",
       " 0.005099559957260188,\n",
       " 0.005059754638750328,\n",
       " 0.005020536030674202,\n",
       " 0.00498189152705561,\n",
       " 0.0049438088761478765,\n",
       " 0.004906276168176602,\n",
       " 0.004869281823585237,\n",
       " 0.004832814581759293,\n",
       " 0.0047968634902065595,\n",
       " 0.004761417894173137,\n",
       " 0.004726467426673171,\n",
       " 0.004692001998915512,\n",
       " 0.004658011791106651,\n",
       " 0.004624487243615008,\n",
       " 0.004591419048479126,\n",
       " 0.004558798141243983,\n",
       " 0.004526615693111493,\n",
       " 0.004494863103391758,\n",
       " 0.004463531992239766,\n",
       " 0.004432614193666866,\n",
       " 0.004402101748815337,\n",
       " 0.004371986899482173,\n",
       " 0.004342262081884165,\n",
       " 0.00431291992065248,\n",
       " 0.004283953223047294,\n",
       " 0.004255354973382484,\n",
       " 0.00422711832765358,\n",
       " 0.0041992366083573734,\n",
       " 0.004171703299498503,\n",
       " 0.004144512041772719,\n",
       " 0.004117656627920653,\n",
       " 0.004091130998245622,\n",
       " 0.004064929236287665,\n",
       " 0.004039045564648131,\n",
       " 0.004013474340958937,\n",
       " 0.003988210053989949,\n",
       " 0.003963247319890445,\n",
       " 0.003938580878557269,\n",
       " 0.00391420559012677,\n",
       " 0.003890116431584271,\n",
       " 0.003866308493487156,\n",
       " 0.003842776976797196,\n",
       " 0.0038195171898175433,\n",
       " 0.0037965245452305668,\n",
       " 0.003773794557232673,\n",
       " 0.0037513228387628487,\n",
       " 0.003729105098820751,\n",
       " 0.0037071371398710194,\n",
       " 0.0036854148553314464,\n",
       " 0.0036639342271408736,\n",
       " 0.0036426913234043168,\n",
       " 0.0036216822961127157,\n",
       " 0.003600903378933896,\n",
       " 0.003580350885073308,\n",
       " 0.003560021205200529,\n",
       " 0.003539910805440503,\n",
       " 0.0035200162254262857]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(vinayTrainX, vinayTrainY, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testoneHotEncoder(y, n_classes):\n",
    "    \"\"\"\n",
    "        One hot encoder\n",
    "        y: input\n",
    "        return: encoded output\n",
    "        \"\"\"\n",
    "    m = y.shape[0]\n",
    "    y_oht = np.zeros((m, n_classes))\n",
    "    y_oht[np.arange(m), y] = 1\n",
    "    return y_oht\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_2206/2879196089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestoneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvinayTrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_2206/1034160486.py\u001b[0m in \u001b[0;36mtestoneHotEncoder\u001b[0;34m(y, n_classes)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_oht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0my_oht\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_oht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "oh = testoneHotEncoder(vinayTrainY, 1)\n",
    "oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
