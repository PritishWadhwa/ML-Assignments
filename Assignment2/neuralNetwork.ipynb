{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork:\n",
    "    def __init__(self, N_inputs, N_outputs, N_layers=2, Layer_sizes=[10, 5], activation=\"sigmoid\", learning_rate=0.1, weight_init=\"random\", batch_size=1, num_epochs=200):\n",
    "        \"\"\"\n",
    "        N_inputs: input size\n",
    "        N_outputs: outputs size\n",
    "        N_layers: number of hidden layers\n",
    "        Layer_sizes: list of hidden layer sizes\n",
    "        activation: activation function to be used (ReLu, Leaky ReLu, sigmoid, linear, tanh, softmax)\n",
    "        learning_rate: learning rate\n",
    "        weight_init: weight initialization (zero, random, normal)\n",
    "        batch_size: batch size\n",
    "        num_epochs: number of epochs\n",
    "        \"\"\"\n",
    "        self.N_inputs = N_inputs\n",
    "        self.N_outputs = N_outputs\n",
    "        self.N_layers = N_layers\n",
    "        self.Layer_sizes = Layer_sizes\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_init = weight_init\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        # np.random.seed()\n",
    "\n",
    "        model = {}\n",
    "        if weight_init == \"zero\":\n",
    "            model['W1'] = np.zeros((N_inputs, Layer_sizes[0]))\n",
    "            model['b1'] = np.zeros((1, Layer_sizes[0]))\n",
    "            for i in range(1, N_layers):\n",
    "                model['W' + str(i+1)] = np.zeros((Layer_sizes[i-1], Layer_sizes[i]))\n",
    "                model['b' + str(i+1)] = np.zeros((1, Layer_sizes[i]))\n",
    "            model['W' + str(N_layers+1)] = np.zeros((Layer_sizes[-1], N_outputs))\n",
    "            model['b' + str(N_layers+1)] = np.zeros((1, N_outputs))\n",
    "        elif weight_init == \"random\":\n",
    "            model['W1'] = np.random.randn(N_inputs, Layer_sizes[0])*0.01\n",
    "            model['b1'] = np.zeros((1, Layer_sizes[0]))\n",
    "            for i in range(1, N_layers):\n",
    "                model['W' + str(i+1)] = np.random.randn(Layer_sizes[i-1],Layer_sizes[i])*0.01\n",
    "                model['b' + str(i+1)] = np.zeros((1, Layer_sizes[i]))\n",
    "            model['W' + str(N_layers+1)\n",
    "                  ] = np.random.randn(Layer_sizes[-1], N_outputs)*0.01\n",
    "            model['b' + str(N_layers+1)] = np.zeros((1, N_outputs))\n",
    "        elif weight_init == \"normal\":\n",
    "            model['W1'] = np.random.normal(0, 1, (N_inputs, Layer_sizes[0]))*0.01\n",
    "            model['b1'] = np.zeros((1, Layer_sizes[0]))\n",
    "            for i in range(1, N_layers):\n",
    "                model['W' + str(i+1)] = np.random.normal(0, 1, (Layer_sizes[i-1], Layer_sizes[i]))*0.01\n",
    "                model['b' + str(i+1)] = np.zeros((1, Layer_sizes[i]))\n",
    "            model['W' + str(N_layers+1)] = np.random.normal(0, 1, (Layer_sizes[-1], N_outputs))*0.01\n",
    "            model['b' + str(N_layers+1)] = np.zeros((1, N_outputs))\n",
    "        else:\n",
    "            print(\"Invalid weight initialization\")\n",
    "            return\n",
    "\n",
    "        self.model = model\n",
    "        self.activationOutputs = None\n",
    "\n",
    "    def relu_forward(self, X):\n",
    "        \"\"\"\n",
    "        ReLu activation function for forward propagation\n",
    "        X: input\n",
    "        return: output after applying the relu function\n",
    "        \"\"\"\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "    def relu_backward(self, X):\n",
    "        \"\"\"\n",
    "        ReLu activation function for backpropagation\n",
    "        X: input\n",
    "        return: output after applying the gradient of relu function\n",
    "        \"\"\"\n",
    "        return np.where(X > 0, 1, 0)\n",
    "\n",
    "    def leaky_relu_forward(self, X):\n",
    "        \"\"\"\n",
    "        Leaky ReLu activation function\n",
    "        X: input\n",
    "        return: output after applying the Leaky ReLu function\n",
    "        \"\"\"\n",
    "        return np.maximum(X, 0.01*X)\n",
    "\n",
    "    def leaky_relu_backward(self, X):\n",
    "        \"\"\"\n",
    "        Leaky ReLu activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of Leaky ReLu function\n",
    "        \"\"\"\n",
    "        return np.where(X > 0, 1, 0.01)\n",
    "\n",
    "    def sigmoid_forward(self, X):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function\n",
    "        X: input\n",
    "        return: output after applying the sigmoid function\n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-X))\n",
    "\n",
    "    def sigmoid_backward(self, X):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of sigmoid function\n",
    "        \"\"\"\n",
    "        return self.sigmoid_forward(X)*(1-self.sigmoid_forward(X))\n",
    "        # return X*(1-X)\n",
    "\n",
    "    def linear_forward(self, X):\n",
    "        \"\"\"\n",
    "        Linear activation function\n",
    "        X: input\n",
    "        return: output after applying the linear function\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def linear_backward(self, X):\n",
    "        \"\"\"\n",
    "        Linear activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of linear function\n",
    "        \"\"\"\n",
    "        return np.ones(X.shape)\n",
    "\n",
    "    def tanh_forward(self, X):\n",
    "        \"\"\"\n",
    "        Tanh activation function\n",
    "        X: input\n",
    "        return: output after applying the tanh function\n",
    "        \"\"\"\n",
    "        return (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
    "\n",
    "    def tanh_backward(self, X):\n",
    "        \"\"\"\n",
    "        Tanh activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of tanh function\n",
    "        \"\"\"\n",
    "        return 1-(self.tanh_forward(X)**2)\n",
    "        # return 1-X**2\n",
    "\n",
    "    def softmax_forward(self, X):\n",
    "        # to be changed\n",
    "        \"\"\"\n",
    "        Softmax activation function\n",
    "        X: input\n",
    "        return: output after applying the softmax function\n",
    "        \"\"\"\n",
    "        exp = np.exp(X - np.max(X))\n",
    "        return exp/np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    # softmax implemetatation incomplete\n",
    "    def softmax_backward(self, X):\n",
    "        \"\"\"\n",
    "        Softmax activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of softmax function\n",
    "        \"\"\"\n",
    "        # softmax = self.softmax_forward(X)\n",
    "        # e = np.ones((softmax.shape[0], 1))\n",
    "        # v1 = np.dot(softmax, e.T)\n",
    "        # i = np.eye(softmax.shape[0])\n",
    "        # v2 = i - np.dot(e, softmax.T)\n",
    "        # return np.multiply(v1, v2)\n",
    "        s = X.reshape(-1, 1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        X: input\n",
    "        return: output after applying the activation function\n",
    "        \"\"\"\n",
    "        if self.activation == \"relu\":\n",
    "            currentActivationFuntion = self.relu_forward\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            currentActivationFuntion = self.leaky_relu_forward\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            currentActivationFuntion = self.sigmoid_forward\n",
    "        elif self.activation == \"linear\":\n",
    "            currentActivationFuntion = self.linear_forward\n",
    "        elif self.activation == \"tanh\":\n",
    "            currentActivationFuntion = self.tanh_forward\n",
    "        elif self.activation == \"softmax\":\n",
    "            currentActivationFuntion = self.softmax_forward\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "        self.activationOutputs = {}\n",
    "\n",
    "        self.activationOutputs['Z1'] = np.dot(X, self.model['W1']) + self.model['b1']\n",
    "        self.activationOutputs['A1'] = currentActivationFuntion(self.activationOutputs['Z1'])\n",
    "        # self.activationOutputs['A1'] = np.tanh(self.activationOutputs['Z1'])\n",
    "\n",
    "        for i in range(2, self.N_layers+1):\n",
    "            self.activationOutputs['Z' + str(i)] = np.dot(self.activationOutputs['A' + str(i-1)], self.model['W' + str(i)]) + self.model['b' + str(i)]\n",
    "            self.activationOutputs['A' + str(i)] = currentActivationFuntion(self.activationOutputs['Z' + str(i)])\n",
    "\n",
    "        self.activationOutputs['Z' + str(self.N_layers+1)] = np.dot(self.activationOutputs['A' + str(\n",
    "            self.N_layers)], self.model['W' + str(self.N_layers+1)]) + self.model['b' + str(self.N_layers+1)]\n",
    "        self.activationOutputs['A' + str(self.N_layers+1)] = currentActivationFuntion(\n",
    "            self.activationOutputs['Z' + str(self.N_layers+1)])\n",
    "\n",
    "        return self.activationOutputs['A' + str(self.N_layers+1)]\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Backward propagation\n",
    "        X: input\n",
    "        Y: output\n",
    "        \"\"\"\n",
    "        if self.activation == \"relu\":\n",
    "            currentActivationFuntion = self.relu_backward\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            currentActivationFuntion = self.leaky_relu_backward\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            currentActivationFuntion = self.sigmoid_backward\n",
    "        elif self.activation == \"linear\":\n",
    "            currentActivationFuntion = self.linear_backward\n",
    "        elif self.activation == \"tanh\":\n",
    "            currentActivationFuntion = self.tanh_backward\n",
    "        elif self.activation == \"softmax\":\n",
    "            currentActivationFuntion = self.softmax_backward\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "        # computing the gradients\n",
    "        self.gradients = {}\n",
    "        self.gradients['delta' + str(self.N_layers+1)] = (self.activationOutputs['A' + str(self.N_layers+1)] - Y)\n",
    "        self.gradients['dW' + str(self.N_layers+1)] = (1/len(X)) * np.dot(self.activationOutputs['A' + str(\n",
    "            self.N_layers)].T, self.gradients['delta' + str(self.N_layers+1)])\n",
    "        self.gradients['db' + str(self.N_layers+1)] = (1/len(X)) * np.sum(\n",
    "            self.gradients['delta' + str(self.N_layers+1)], axis=0, keepdims=True)\n",
    "\n",
    "        for i in range(self.N_layers, 1, -1):\n",
    "            self.gradients['delta' + str(i)] = np.dot(self.gradients['delta' + str(i+1)], self.model['W' + str(i+1)].T) * currentActivationFuntion(self.activationOutputs['Z' + str(i)])\n",
    "            self.gradients['dW' + str(i)] = (1/len(X)) * np.dot(self.activationOutputs['A' + str(i-1)].T, self.gradients['delta' + str(i)])\n",
    "            self.gradients['db' + str(i)] = (1/len(X)) * np.sum(\n",
    "                self.gradients['delta' + str(i)], axis=0, keepdims=True)\n",
    "\n",
    "        self.gradients['delta1'] = np.dot(self.gradients['delta2'], self.model['W2'].T) * currentActivationFuntion(self.activationOutputs['Z1'])\n",
    "        self.gradients['dW1'] = (1/len(X)) * np.dot(X.T, self.gradients['delta1'])\n",
    "        self.gradients['db1'] = (1/len(X)) * np.sum(self.gradients['delta1'], axis=0, keepdims=True)\n",
    "\n",
    "        # updating the model parameters\n",
    "        for i in range(1, self.N_layers+2):\n",
    "            self.model['W' + str(i)] -= self.learning_rate * self.gradients['dW' + str(i)]\n",
    "            self.model['b' + str(i)] -= self.learning_rate * self.gradients['db' + str(i)]\n",
    "\n",
    "    def oneHotEncoder(self, y, n_classes):\n",
    "        \"\"\"\n",
    "        One hot encoder\n",
    "        y: input\n",
    "        return: encoded output\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        y_oht = np.zeros((m, n_classes))\n",
    "        y_oht[np.arange(m), y] = 1\n",
    "        return y_oht\n",
    "\n",
    "    def crossEntropyLoss(self, y_oht, y_prob):\n",
    "        \"\"\"\n",
    "        Cross entropy loss\n",
    "        y_oht: one hot encoded output\n",
    "        y_prob: probabilities for classes\n",
    "        return: cross entropy loss\n",
    "        \"\"\"\n",
    "        return -np.mean(y_oht * np.log(y_prob + 1e-8))\n",
    "\n",
    "    def fit(self, X, y, validX=None, validY=None, logs=True):\n",
    "        \"\"\"\n",
    "        Fit the model to the data\n",
    "        X: input\n",
    "        Y: output\n",
    "        epochs: number of epochs\n",
    "        \"\"\"\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        classes = self.N_outputs\n",
    "        batchSize = self.batch_size\n",
    "        y_oht = self.oneHotEncoder(y, classes)\n",
    "        if validX is not None and validY is not None:\n",
    "            y_oht_valid = self.oneHotEncoder(validY, classes)\n",
    "        for i in range(self.num_epochs):\n",
    "            for j in range(0, X.shape[0], batchSize):\n",
    "                X_batch = X[j:j+batchSize]\n",
    "                y_batch = y_oht[j:j+batchSize]\n",
    "                y_ = self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch)\n",
    "            y_ = self.forward(X)\n",
    "            train_loss = self.crossEntropyLoss(y_oht, y_)\n",
    "            train_losses.append(train_loss)\n",
    "            if validX is not None and validY is not None:\n",
    "                y_valid = self.forward(validX)\n",
    "                valid_loss = self.crossEntropyLoss(y_oht_valid, y_valid)\n",
    "                valid_losses.append(valid_loss)\n",
    "            if logs:\n",
    "                print(\"Epoch: {}, Loss: {}, Score: {}\".format(i, train_loss, self.score(X, y)))\n",
    "        if validX is not None and validY is not None:\n",
    "            return train_losses, valid_losses\n",
    "        return train_losses\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probabilities\n",
    "        X: input\n",
    "        return: probabilities\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes\n",
    "        X: input\n",
    "        return: classes\n",
    "        \"\"\"\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Score the model\n",
    "        X: input\n",
    "        Y: output\n",
    "        return: accuracy\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "allX, ally = load_mnist('Weights/Ques2/', kind='train')\n",
    "allX_2, ally_2 = load_mnist('Weights/Ques2/', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = allX\n",
    "X = np.concatenate((X, allX_2), axis=0)\n",
    "y = ally\n",
    "y = np.concatenate((y, ally_2), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, trainSize=0.8, testSize=0.2, random_state=1):\n",
    "    validSize = 1 - trainSize - testSize\n",
    "    indices = np.arange(df.shape[0])\n",
    "    # np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    trainData = df.iloc[indices[:int(\n",
    "        trainSize*df.shape[0])]].reset_index(drop=True)\n",
    "    validData = df.iloc[indices[int(\n",
    "        trainSize*df.shape[0]):int((trainSize+validSize)*df.shape[0])]].reset_index(drop=True)\n",
    "    testData = df.iloc[indices[int(\n",
    "        (trainSize+validSize)*df.shape[0]):]].reset_index(drop=True)\n",
    "    if validSize == 0:\n",
    "        return trainData, testData\n",
    "    else:\n",
    "        return trainData, validData, testData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   779  780  781  782  783  0    \n",
       "0    0    0    0    0    0    5  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    4  \n",
       "3    0    0    0    0    0    1  \n",
       "4    0    0    0    0    0    9  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)\n",
    "allData = pd.concat([X, y], axis=1)\n",
    "print(allData.shape)\n",
    "allData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData = train_test_split(allData, trainSize=0.7, testSize=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainData.iloc[:, :-1]\n",
    "trainY = trainData.iloc[:, -1]\n",
    "validX = validData.iloc[:, :-1]\n",
    "validY = validData.iloc[:, -1]\n",
    "testX = testData.iloc[:, :-1]\n",
    "testY = testData.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX = preprocessing.normalize(trainX)\n",
    "# validX = preprocessing.normalize(validX)\n",
    "# testX = preprocessing.normalize(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.to_numpy()\n",
    "validX = validX.to_numpy()\n",
    "testX = testX.to_numpy()\n",
    "trainY = trainY.to_numpy()\n",
    "validY = validY.to_numpy()\n",
    "testY = testY.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX/255\n",
    "validX = validX/255\n",
    "testX = testX/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "trainX = ss.fit_transform(trainX)\n",
    "validX = ss.transform(validX)\n",
    "testX = ss.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNeuralNetwork(N_inputs=784, N_outputs=10, N_layers=1, Layer_sizes=[100], activation=\"sigmoid\", learning_rate=0.1, weight_init=\"normal\", num_epochs=150, batch_size=len(X)//20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.22665805920426005, Score: 11.369387755102041\n",
      "Epoch: 1, Loss: 0.22594071700482432, Score: 13.77142857142857\n",
      "Epoch: 2, Loss: 0.2208536010977268, Score: 28.9265306122449\n",
      "Epoch: 3, Loss: 0.2112741870408808, Score: 41.891836734693875\n",
      "Epoch: 4, Loss: 0.19653954009051727, Score: 48.234693877551024\n",
      "Epoch: 5, Loss: 0.17861495060494956, Score: 53.68367346938776\n",
      "Epoch: 6, Loss: 0.1607306012358383, Score: 58.78775510204082\n",
      "Epoch: 7, Loss: 0.14506535718606894, Score: 63.76326530612245\n",
      "Epoch: 8, Loss: 0.1321827693522129, Score: 68.02244897959183\n",
      "Epoch: 9, Loss: 0.12169897194643386, Score: 71.44081632653061\n",
      "Epoch: 10, Loss: 0.11300894742071257, Score: 74.28163265306122\n",
      "Epoch: 11, Loss: 0.10561829258248953, Score: 76.58571428571429\n",
      "Epoch: 12, Loss: 0.09919737795627308, Score: 78.41836734693878\n",
      "Epoch: 13, Loss: 0.09354184423638748, Score: 79.91428571428571\n",
      "Epoch: 14, Loss: 0.08852498478116387, Score: 81.20408163265306\n",
      "Epoch: 15, Loss: 0.08406273003580447, Score: 82.2204081632653\n",
      "Epoch: 16, Loss: 0.08009155879052966, Score: 83.02244897959183\n",
      "Epoch: 17, Loss: 0.076556562804675, Score: 83.7530612244898\n",
      "Epoch: 18, Loss: 0.0734066491116641, Score: 84.4469387755102\n",
      "Epoch: 19, Loss: 0.07059378218833644, Score: 85.06326530612245\n",
      "Epoch: 20, Loss: 0.06807374716023258, Score: 85.55102040816327\n",
      "Epoch: 21, Loss: 0.06580698209654315, Score: 85.97142857142858\n",
      "Epoch: 22, Loss: 0.06375895691849677, Score: 86.3734693877551\n",
      "Epoch: 23, Loss: 0.06190008444617628, Score: 86.76530612244898\n",
      "Epoch: 24, Loss: 0.06020531878272323, Score: 87.10408163265306\n",
      "Epoch: 25, Loss: 0.058653600632727454, Score: 87.46122448979592\n",
      "Epoch: 26, Loss: 0.05722726368186756, Score: 87.7265306122449\n",
      "Epoch: 27, Loss: 0.0559114698207154, Score: 87.95918367346938\n",
      "Epoch: 28, Loss: 0.05469370695480346, Score: 88.20408163265306\n",
      "Epoch: 29, Loss: 0.05356336143478527, Score: 88.39591836734694\n",
      "Epoch: 30, Loss: 0.05251136462871647, Score: 88.61428571428571\n",
      "Epoch: 31, Loss: 0.05152990674907773, Score: 88.77142857142857\n",
      "Epoch: 32, Loss: 0.0506122084044345, Score: 88.93469387755101\n",
      "Epoch: 33, Loss: 0.049752339873291085, Score: 89.08979591836734\n",
      "Epoch: 34, Loss: 0.04894507876174765, Score: 89.2265306122449\n",
      "Epoch: 35, Loss: 0.04818579787751841, Score: 89.32857142857142\n",
      "Epoch: 36, Loss: 0.04747037648378877, Score: 89.43877551020408\n",
      "Epoch: 37, Loss: 0.046795129411286294, Score: 89.56326530612245\n",
      "Epoch: 38, Loss: 0.04615674971093776, Score: 89.66326530612245\n",
      "Epoch: 39, Loss: 0.04555226156588813, Score: 89.7734693877551\n",
      "Epoch: 40, Loss: 0.044978981024736985, Score: 89.87142857142857\n",
      "Epoch: 41, Loss: 0.04443448277065117, Score: 89.96326530612245\n",
      "Epoch: 42, Loss: 0.043916571625879704, Score: 90.10204081632654\n",
      "Epoch: 43, Loss: 0.04342325783872806, Score: 90.2\n",
      "Epoch: 44, Loss: 0.04295273544255527, Score: 90.2673469387755\n",
      "Epoch: 45, Loss: 0.042503363145846046, Score: 90.33265306122449\n",
      "Epoch: 46, Loss: 0.042073647339723304, Score: 90.41428571428571\n",
      "Epoch: 47, Loss: 0.0416622269195149, Score: 90.4591836734694\n",
      "Epoch: 48, Loss: 0.04126785971828821, Score: 90.55102040816327\n",
      "Epoch: 49, Loss: 0.04088941042492225, Score: 90.62857142857142\n",
      "Epoch: 50, Loss: 0.04052583988051323, Score: 90.68163265306123\n",
      "Epoch: 51, Loss: 0.0401761956189658, Score: 90.76122448979592\n",
      "Epoch: 52, Loss: 0.03983960348509364, Score: 90.8061224489796\n",
      "Epoch: 53, Loss: 0.03951526016514564, Score: 90.84897959183674\n",
      "Epoch: 54, Loss: 0.03920242649674176, Score: 90.93469387755103\n",
      "Epoch: 55, Loss: 0.03890042146116236, Score: 90.96530612244898\n",
      "Epoch: 56, Loss: 0.03860861678449843, Score: 91.03877551020409\n",
      "Epoch: 57, Loss: 0.03832643208562654, Score: 91.10204081632654\n",
      "Epoch: 58, Loss: 0.038053330514639175, Score: 91.15510204081633\n",
      "Epoch: 59, Loss: 0.037788814829926534, Score: 91.19591836734693\n",
      "Epoch: 60, Loss: 0.03753242386734921, Score: 91.22857142857143\n",
      "Epoch: 61, Loss: 0.03728372936089173, Score: 91.25714285714285\n",
      "Epoch: 62, Loss: 0.03704233308020477, Score: 91.31020408163265\n",
      "Epoch: 63, Loss: 0.036807864255740334, Score: 91.36326530612246\n",
      "Epoch: 64, Loss: 0.0365799772660582, Score: 91.42244897959185\n",
      "Epoch: 65, Loss: 0.03635834956381274, Score: 91.46938775510203\n",
      "Epoch: 66, Loss: 0.03614267981668768, Score: 91.52244897959184\n",
      "Epoch: 67, Loss: 0.035932686237408334, Score: 91.5673469387755\n",
      "Epoch: 68, Loss: 0.035728105074052476, Score: 91.62244897959184\n",
      "Epoch: 69, Loss: 0.0355286892304522, Score: 91.65510204081633\n",
      "Epoch: 70, Loss: 0.03533420698936528, Score: 91.68775510204081\n",
      "Epoch: 71, Loss: 0.0351444408198947, Score: 91.71632653061225\n",
      "Epoch: 72, Loss: 0.03495918626335437, Score: 91.74081632653062\n",
      "Epoch: 73, Loss: 0.0347782509026738, Score: 91.78775510204082\n",
      "Epoch: 74, Loss: 0.034601453423813655, Score: 91.8469387755102\n",
      "Epoch: 75, Loss: 0.03442862277298507, Score: 91.86530612244897\n",
      "Epoch: 76, Loss: 0.03425959740594138, Score: 91.89591836734694\n",
      "Epoch: 77, Loss: 0.03409422462092255, Score: 91.93469387755103\n",
      "Epoch: 78, Loss: 0.033932359965826164, Score: 91.9673469387755\n",
      "Epoch: 79, Loss: 0.033773866709447695, Score: 92.01020408163265\n",
      "Epoch: 80, Loss: 0.033618615363532826, Score: 92.05714285714286\n",
      "Epoch: 81, Loss: 0.03346648323847576, Score: 92.10204081632654\n",
      "Epoch: 82, Loss: 0.033317354015132805, Score: 92.12857142857143\n",
      "Epoch: 83, Loss: 0.03317111732142521, Score: 92.15102040816326\n",
      "Epoch: 84, Loss: 0.03302766831376937, Score: 92.17551020408163\n",
      "Epoch: 85, Loss: 0.03288690727438943, Score: 92.20816326530613\n",
      "Epoch: 86, Loss: 0.032748739240293395, Score: 92.24489795918367\n",
      "Epoch: 87, Loss: 0.03261307367606883, Score: 92.2673469387755\n",
      "Epoch: 88, Loss: 0.032479824193773724, Score: 92.29183673469387\n",
      "Epoch: 89, Loss: 0.032348908314513224, Score: 92.33061224489796\n",
      "Epoch: 90, Loss: 0.032220247261604426, Score: 92.35918367346939\n",
      "Epoch: 91, Loss: 0.032093765775034026, Score: 92.38367346938774\n",
      "Epoch: 92, Loss: 0.03196939193944924, Score: 92.41428571428571\n",
      "Epoch: 93, Loss: 0.03184705702112726, Score: 92.43265306122449\n",
      "Epoch: 94, Loss: 0.03172669531195547, Score: 92.4591836734694\n",
      "Epoch: 95, Loss: 0.03160824398004385, Score: 92.48571428571428\n",
      "Epoch: 96, Loss: 0.03149164292731512, Score: 92.51224489795918\n",
      "Epoch: 97, Loss: 0.031376834654584795, Score: 92.52448979591836\n",
      "Epoch: 98, Loss: 0.03126376413451645, Score: 92.54897959183673\n",
      "Epoch: 99, Loss: 0.031152378692587874, Score: 92.57142857142857\n",
      "Epoch: 100, Loss: 0.031042627895905114, Score: 92.59387755102041\n",
      "Epoch: 101, Loss: 0.030934463449352495, Score: 92.62857142857143\n",
      "Epoch: 102, Loss: 0.03082783909812265, Score: 92.65102040816326\n",
      "Epoch: 103, Loss: 0.030722710535098424, Score: 92.67755102040816\n",
      "Epoch: 104, Loss: 0.03061903531090427, Score: 92.69795918367348\n",
      "Epoch: 105, Loss: 0.030516772743927542, Score: 92.71428571428572\n",
      "Epoch: 106, Loss: 0.030415883827627856, Score: 92.7326530612245\n",
      "Epoch: 107, Loss: 0.03031633113343169, Score: 92.7469387755102\n",
      "Epoch: 108, Loss: 0.030218078709576945, Score: 92.76326530612245\n",
      "Epoch: 109, Loss: 0.030121091978930377, Score: 92.78979591836735\n",
      "Epoch: 110, Loss: 0.030025337640957807, Score: 92.8265306122449\n",
      "Epoch: 111, Loss: 0.029930783583551817, Score: 92.85102040816327\n",
      "Epoch: 112, Loss: 0.029837398808894498, Score: 92.87551020408164\n",
      "Epoch: 113, Loss: 0.0297451533745937, Score: 92.89591836734694\n",
      "Epoch: 114, Loss: 0.029654018348264862, Score: 92.91632653061225\n",
      "Epoch: 115, Loss: 0.02956396577168013, Score: 92.9469387755102\n",
      "Epoch: 116, Loss: 0.029474968630029963, Score: 92.96122448979592\n",
      "Epoch: 117, Loss: 0.02938700082252047, Score: 92.9734693877551\n",
      "Epoch: 118, Loss: 0.029300037131899857, Score: 92.98979591836735\n",
      "Epoch: 119, Loss: 0.02921405319200623, Score: 93.01020408163265\n",
      "Epoch: 120, Loss: 0.029129025453657316, Score: 93.03469387755102\n",
      "Epoch: 121, Loss: 0.029044931149953206, Score: 93.06326530612245\n",
      "Epoch: 122, Loss: 0.028961748262299795, Score: 93.08571428571429\n",
      "Epoch: 123, Loss: 0.028879455488272886, Score: 93.11632653061224\n",
      "Epoch: 124, Loss: 0.02879803221200219, Score: 93.12857142857143\n",
      "Epoch: 125, Loss: 0.02871745847725562, Score: 93.14693877551021\n",
      "Epoch: 126, Loss: 0.02863771496299253, Score: 93.16122448979591\n",
      "Epoch: 127, Loss: 0.028558782960912677, Score: 93.18571428571428\n",
      "Epoch: 128, Loss: 0.028480644354458292, Score: 93.2\n",
      "Epoch: 129, Loss: 0.028403281598785916, Score: 93.21428571428572\n",
      "Epoch: 130, Loss: 0.028326677701355232, Score: 93.23469387755102\n",
      "Epoch: 131, Loss: 0.028250816202927156, Score: 93.2469387755102\n",
      "Epoch: 132, Loss: 0.028175681158890516, Score: 93.26326530612245\n",
      "Epoch: 133, Loss: 0.0281012571209272, Score: 93.29591836734694\n",
      "Epoch: 134, Loss: 0.028027529119078, Score: 93.31224489795919\n",
      "Epoch: 135, Loss: 0.027954482644291182, Score: 93.32040816326531\n",
      "Epoch: 136, Loss: 0.02788210363153263, Score: 93.34285714285714\n",
      "Epoch: 137, Loss: 0.0278103784435191, Score: 93.35510204081633\n",
      "Epoch: 138, Loss: 0.027739293855113607, Score: 93.36326530612244\n",
      "Epoch: 139, Loss: 0.027668837038398283, Score: 93.38979591836734\n",
      "Epoch: 140, Loss: 0.027598995548420845, Score: 93.40612244897959\n",
      "Epoch: 141, Loss: 0.027529757309594626, Score: 93.43061224489796\n",
      "Epoch: 142, Loss: 0.027461110602722664, Score: 93.44081632653061\n",
      "Epoch: 143, Loss: 0.02739304405260996, Score: 93.4591836734694\n",
      "Epoch: 144, Loss: 0.027325546616225826, Score: 93.47755102040817\n",
      "Epoch: 145, Loss: 0.02725860757137895, Score: 93.49795918367347\n",
      "Epoch: 146, Loss: 0.027192216505868606, Score: 93.51428571428572\n",
      "Epoch: 147, Loss: 0.02712636330707991, Score: 93.52448979591837\n",
      "Epoch: 148, Loss: 0.027061038151992744, Score: 93.5326530612245\n",
      "Epoch: 149, Loss: 0.02699623149757849, Score: 93.54285714285714\n"
     ]
    }
   ],
   "source": [
    "trainLoss, validLoss = model.fit(trainX, trainY, validX=validX, validY=validY, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.37142857142857"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainLoss, label='Training Loss')\n",
    "plt.plot(validLoss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
