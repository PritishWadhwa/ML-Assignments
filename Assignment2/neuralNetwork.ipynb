{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork:\n",
    "    def __init__(self, N_inputs, N_outputs, N_layers=2, Layer_sizes=[10, 5], activation=\"sigmoid\", learning_rate=0.1, weight_init=\"random\", batch_size=1, num_epochs=200):\n",
    "        \"\"\"\n",
    "        N_inputs: input size\n",
    "        N_outputs: outputs size\n",
    "        N_layers: number of hidden layers\n",
    "        Layer_sizes: list of hidden layer sizes\n",
    "        activation: activation function to be used (ReLu, Leaky ReLu, sigmoid, linear, tanh, softmax)\n",
    "        learning_rate: learning rate\n",
    "        weight_init: weight initialization (zero, random, normal)\n",
    "        batch_size: batch size\n",
    "        num_epochs: number of epochs\n",
    "        \"\"\"\n",
    "        self.N_inputs = N_inputs\n",
    "        self.N_outputs = N_outputs\n",
    "        self.N_layers = N_layers\n",
    "        self.Layer_sizes = Layer_sizes\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_init = weight_init\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        # np.random.seed()\n",
    "\n",
    "        model = {}\n",
    "        if weight_init == \"zero\":\n",
    "            model['W1'] = np.zeros((N_inputs, Layer_sizes[0]))\n",
    "            model['b1'] = np.zeros((1, Layer_sizes[0]))\n",
    "            for i in range(1, N_layers):\n",
    "                model['W' + str(i+1)] = np.zeros((Layer_sizes[i-1], Layer_sizes[i]))\n",
    "                model['b' + str(i+1)] = np.zeros((1, Layer_sizes[i]))\n",
    "            model['W' + str(N_layers+1)] = np.zeros((Layer_sizes[-1], N_outputs))\n",
    "            model['b' + str(N_layers+1)] = np.zeros((1, N_outputs))\n",
    "        elif weight_init == \"random\":\n",
    "            model['W1'] = np.random.randn(N_inputs, Layer_sizes[0])*0.01\n",
    "            model['b1'] = np.zeros((1, Layer_sizes[0]))\n",
    "            for i in range(1, N_layers):\n",
    "                model['W' + str(i+1)] = np.random.randn(Layer_sizes[i-1],Layer_sizes[i])*0.01\n",
    "                model['b' + str(i+1)] = np.zeros((1, Layer_sizes[i]))\n",
    "            model['W' + str(N_layers+1)\n",
    "                  ] = np.random.randn(Layer_sizes[-1], N_outputs)*0.01\n",
    "            model['b' + str(N_layers+1)] = np.zeros((1, N_outputs))\n",
    "        elif weight_init == \"normal\":\n",
    "            model['W1'] = np.random.normal(0, 1, (N_inputs, Layer_sizes[0]))*0.01\n",
    "            model['b1'] = np.zeros((1, Layer_sizes[0]))\n",
    "            for i in range(1, N_layers):\n",
    "                model['W' + str(i+1)] = np.random.normal(0, 1, (Layer_sizes[i-1], Layer_sizes[i]))*0.01\n",
    "                model['b' + str(i+1)] = np.zeros((1, Layer_sizes[i]))\n",
    "            model['W' + str(N_layers+1)] = np.random.normal(0, 1, (Layer_sizes[-1], N_outputs))*0.01\n",
    "            model['b' + str(N_layers+1)] = np.zeros((1, N_outputs))\n",
    "        else:\n",
    "            print(\"Invalid weight initialization\")\n",
    "            return\n",
    "\n",
    "        self.model = model\n",
    "        self.activationOutputs = None\n",
    "\n",
    "    def relu_forward(self, X):\n",
    "        \"\"\"\n",
    "        ReLu activation function for forward propagation\n",
    "        X: input\n",
    "        return: output after applying the relu function\n",
    "        \"\"\"\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "    def relu_backward(self, X):\n",
    "        \"\"\"\n",
    "        ReLu activation function for backpropagation\n",
    "        X: input\n",
    "        return: output after applying the gradient of relu function\n",
    "        \"\"\"\n",
    "        return np.where(X > 0, 1, 0)\n",
    "\n",
    "    def leaky_relu_forward(self, X):\n",
    "        \"\"\"\n",
    "        Leaky ReLu activation function\n",
    "        X: input\n",
    "        return: output after applying the Leaky ReLu function\n",
    "        \"\"\"\n",
    "        return np.maximum(X, 0.01*X)\n",
    "\n",
    "    def leaky_relu_backward(self, X):\n",
    "        \"\"\"\n",
    "        Leaky ReLu activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of Leaky ReLu function\n",
    "        \"\"\"\n",
    "        return np.where(X > 0, 1, 0.01)\n",
    "\n",
    "    def sigmoid_forward(self, X):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function\n",
    "        X: input\n",
    "        return: output after applying the sigmoid function\n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-X))\n",
    "\n",
    "    def sigmoid_backward(self, X):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of sigmoid function\n",
    "        \"\"\"\n",
    "        return self.sigmoid_forward(X)*(1-self.sigmoid_forward(X))\n",
    "        # return X*(1-X)\n",
    "\n",
    "    def linear_forward(self, X):\n",
    "        \"\"\"\n",
    "        Linear activation function\n",
    "        X: input\n",
    "        return: output after applying the linear function\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def linear_backward(self, X):\n",
    "        \"\"\"\n",
    "        Linear activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of linear function\n",
    "        \"\"\"\n",
    "        return np.ones(X.shape)\n",
    "\n",
    "    def tanh_forward(self, X):\n",
    "        \"\"\"\n",
    "        Tanh activation function\n",
    "        X: input\n",
    "        return: output after applying the tanh function\n",
    "        \"\"\"\n",
    "        # return (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def tanh_backward(self, X):\n",
    "        \"\"\"\n",
    "        Tanh activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of tanh function\n",
    "        \"\"\"\n",
    "        return 1-(self.tanh_forward(X)**2)\n",
    "        # return 1-X**2\n",
    "\n",
    "    def softmax_forward(self, X):\n",
    "        \"\"\"\n",
    "        Softmax activation function\n",
    "        X: input\n",
    "        return: output after applying the softmax function\n",
    "        \"\"\"\n",
    "        exp = np.exp(X - np.max(X))\n",
    "        return exp/np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def softmax_backward_actual(self, X):\n",
    "        \"\"\"\n",
    "        Softmax activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of softmax function\n",
    "        \"\"\"\n",
    "        s = self.softmax_forward(X).reshape(-1, 1)\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    def softmax_backward(self, X):\n",
    "        \"\"\"\n",
    "        Softmax activation function\n",
    "        X: input\n",
    "        return: output after applying the gradient of softmax function\n",
    "        \"\"\"\n",
    "        return self.softmax_forward(X)*(1-self.softmax_forward(X))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        X: input\n",
    "        return: output after applying the activation function\n",
    "        \"\"\"\n",
    "        if self.activation == \"relu\":\n",
    "            currentActivationFuntion = self.relu_forward\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            currentActivationFuntion = self.leaky_relu_forward\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            currentActivationFuntion = self.sigmoid_forward\n",
    "        elif self.activation == \"linear\":\n",
    "            currentActivationFuntion = self.linear_forward\n",
    "        elif self.activation == \"tanh\":\n",
    "            currentActivationFuntion = self.tanh_forward\n",
    "        elif self.activation == \"softmax\":\n",
    "            currentActivationFuntion = self.softmax_forward\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "        self.activationOutputs = {}\n",
    "\n",
    "        self.activationOutputs['Z1'] = np.dot(X, self.model['W1']) + self.model['b1']\n",
    "        self.activationOutputs['A1'] = currentActivationFuntion(self.activationOutputs['Z1'])\n",
    "        # self.activationOutputs['A1'] = np.tanh(self.activationOutputs['Z1'])\n",
    "\n",
    "        for i in range(2, self.N_layers+1):\n",
    "            self.activationOutputs['Z' + str(i)] = np.dot(self.activationOutputs['A' + str(i-1)], self.model['W' + str(i)]) + self.model['b' + str(i)]\n",
    "            self.activationOutputs['A' + str(i)] = currentActivationFuntion(self.activationOutputs['Z' + str(i)])\n",
    "\n",
    "        self.activationOutputs['Z' + str(self.N_layers+1)] = np.dot(self.activationOutputs['A' + str(\n",
    "            self.N_layers)], self.model['W' + str(self.N_layers+1)]) + self.model['b' + str(self.N_layers+1)]\n",
    "        self.activationOutputs['A' + str(self.N_layers+1)] = currentActivationFuntion(\n",
    "            self.activationOutputs['Z' + str(self.N_layers+1)])\n",
    "\n",
    "        return self.activationOutputs['A' + str(self.N_layers+1)]\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Backward propagation\n",
    "        X: input\n",
    "        Y: output\n",
    "        \"\"\"\n",
    "        if self.activation == \"relu\":\n",
    "            currentActivationFuntion = self.relu_backward\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            currentActivationFuntion = self.leaky_relu_backward\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            currentActivationFuntion = self.sigmoid_backward\n",
    "        elif self.activation == \"linear\":\n",
    "            currentActivationFuntion = self.linear_backward\n",
    "        elif self.activation == \"tanh\":\n",
    "            currentActivationFuntion = self.tanh_backward\n",
    "        elif self.activation == \"softmax\":\n",
    "            currentActivationFuntion = self.softmax_backward\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "        # computing the gradients\n",
    "        self.gradients = {}\n",
    "        self.gradients['delta' + str(self.N_layers+1)] = (self.activationOutputs['A' + str(self.N_layers+1)] - Y)\n",
    "        self.gradients['dW' + str(self.N_layers+1)] = (1/len(X)) * np.dot(self.activationOutputs['A' + str(\n",
    "            self.N_layers)].T, self.gradients['delta' + str(self.N_layers+1)])\n",
    "        self.gradients['db' + str(self.N_layers+1)] = (1/len(X)) * np.sum(\n",
    "            self.gradients['delta' + str(self.N_layers+1)], axis=0, keepdims=True)\n",
    "\n",
    "        for i in range(self.N_layers, 1, -1):\n",
    "            self.gradients['delta' + str(i)] = np.dot(self.gradients['delta' + str(i+1)], self.model['W' + str(i+1)].T) * currentActivationFuntion(self.activationOutputs['Z' + str(i)])\n",
    "            self.gradients['dW' + str(i)] = (1/len(X)) * np.dot(self.activationOutputs['A' + str(i-1)].T, self.gradients['delta' + str(i)])\n",
    "            self.gradients['db' + str(i)] = (1/len(X)) * np.sum(\n",
    "                self.gradients['delta' + str(i)], axis=0, keepdims=True)\n",
    "\n",
    "        self.gradients['delta1'] = np.dot(self.gradients['delta2'], self.model['W2'].T) * currentActivationFuntion(self.activationOutputs['Z1'])\n",
    "        self.gradients['dW1'] = (1/len(X)) * np.dot(X.T, self.gradients['delta1'])\n",
    "        self.gradients['db1'] = (1/len(X)) * np.sum(self.gradients['delta1'], axis=0, keepdims=True)\n",
    "\n",
    "        # updating the model parameters\n",
    "        for i in range(1, self.N_layers+2):\n",
    "            self.model['W' + str(i)] -= self.learning_rate * self.gradients['dW' + str(i)]\n",
    "            self.model['b' + str(i)] -= self.learning_rate * self.gradients['db' + str(i)]\n",
    "\n",
    "    def oneHotEncoder(self, y, n_classes):\n",
    "        \"\"\"\n",
    "        One hot encoder\n",
    "        y: input\n",
    "        return: encoded output\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        y_oht = np.zeros((m, n_classes))\n",
    "        y_oht[np.arange(m), y] = 1\n",
    "        return y_oht\n",
    "\n",
    "    def crossEntropyLoss(self, y_oht, y_prob):\n",
    "        \"\"\"\n",
    "        Cross entropy loss\n",
    "        y_oht: one hot encoded output\n",
    "        y_prob: probabilities for classes\n",
    "        return: cross entropy loss\n",
    "        \"\"\"\n",
    "        return -np.mean(y_oht * np.log(y_prob + 1e-8))\n",
    "\n",
    "    def crossEntropyLoss1(self, y, A):\n",
    "        n = len(y)\n",
    "        logp = - np.log(A[np.arange(n), y.argmax(axis=1)])\n",
    "        loss = np.sum(logp)/n\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X, y, validX=None, validY=None, logs=True):\n",
    "        \"\"\"\n",
    "        Fit the model to the data\n",
    "        X: input\n",
    "        Y: output\n",
    "        epochs: number of epochs\n",
    "        \"\"\"\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        classes = self.N_outputs\n",
    "        batchSize = self.batch_size\n",
    "        y_oht = self.oneHotEncoder(y, classes)\n",
    "        if validX is not None and validY is not None:\n",
    "            y_oht_valid = self.oneHotEncoder(validY, classes)\n",
    "        for i in range(self.num_epochs):\n",
    "            for j in range(0, X.shape[0], batchSize):\n",
    "                X_batch = X[j:j+batchSize]\n",
    "                y_batch = y_oht[j:j+batchSize]\n",
    "                y_ = self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch)\n",
    "            y_ = self.forward(X)\n",
    "            train_loss = self.crossEntropyLoss(y_oht, y_)\n",
    "            train_losses.append(train_loss)\n",
    "            if validX is not None and validY is not None:\n",
    "                y_valid = self.forward(validX)\n",
    "                valid_loss = self.crossEntropyLoss(y_oht_valid, y_valid)\n",
    "                valid_losses.append(valid_loss)\n",
    "            if logs:\n",
    "                print(\"Epoch: {}, Loss: {}, Score: {}\".format(i, train_loss, self.score(X, y)))\n",
    "        if validX is not None and validY is not None:\n",
    "            return train_losses, valid_losses\n",
    "        return train_losses\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probabilities\n",
    "        X: input\n",
    "        return: probabilities\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes\n",
    "        X: input\n",
    "        return: classes\n",
    "        \"\"\"\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Score the model\n",
    "        X: input\n",
    "        Y: output\n",
    "        return: accuracy\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)*100\n",
    "\n",
    "\n",
    "    def saveWeights(self, filename):\n",
    "        \"\"\"\n",
    "        Save the weights\n",
    "        filename: name of the file\n",
    "        \"\"\"\n",
    "        np.save(filename, self.model, allow_pickle=True)\n",
    "\n",
    "    def loadWeights(self, filename):\n",
    "        \"\"\"\n",
    "        Load the weights\n",
    "        filename: name of the file\n",
    "        \"\"\"\n",
    "        self.model = np.load(filename, allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "allX, ally = load_mnist('Weights/Ques2/', kind='train')\n",
    "allX_2, ally_2 = load_mnist('Weights/Ques2/', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = allX\n",
    "X = np.concatenate((X, allX_2), axis=0)\n",
    "y = ally\n",
    "y = np.concatenate((y, ally_2), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, trainSize=0.8, testSize=0.2, random_state=1):\n",
    "    validSize = 1 - trainSize - testSize\n",
    "    indices = np.arange(df.shape[0])\n",
    "    # np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    trainData = df.iloc[indices[:int(\n",
    "        trainSize*df.shape[0])]].reset_index(drop=True)\n",
    "    validData = df.iloc[indices[int(\n",
    "        trainSize*df.shape[0]):int((trainSize+validSize)*df.shape[0])]].reset_index(drop=True)\n",
    "    testData = df.iloc[indices[int(\n",
    "        (trainSize+validSize)*df.shape[0]):]].reset_index(drop=True)\n",
    "    if validSize == 0:\n",
    "        return trainData, testData\n",
    "    else:\n",
    "        return trainData, validData, testData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   779  780  781  782  783  0    \n",
       "0    0    0    0    0    0    5  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    4  \n",
       "3    0    0    0    0    0    1  \n",
       "4    0    0    0    0    0    9  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(X)\n",
    "y = pd.DataFrame(y)\n",
    "allData = pd.concat([X, y], axis=1)\n",
    "print(allData.shape)\n",
    "allData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, validData, testData = train_test_split(allData, trainSize=0.7, testSize=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainData.iloc[:, :-1]\n",
    "trainY = trainData.iloc[:, -1]\n",
    "validX = validData.iloc[:, :-1]\n",
    "validY = validData.iloc[:, -1]\n",
    "testX = testData.iloc[:, :-1]\n",
    "testY = testData.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = preprocessing.normalize(trainX)\n",
    "validX = preprocessing.normalize(validX)\n",
    "testX = preprocessing.normalize(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX = trainX.to_numpy()\n",
    "# validX = validX.to_numpy()\n",
    "# testX = testX.to_numpy()\n",
    "# trainY = trainY.to_numpy()\n",
    "# validY = validY.to_numpy()\n",
    "# testY = testY.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX/255\n",
    "validX = validX/255\n",
    "testX = testX/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainMean = np.mean(trainX)\n",
    "# trainStd = np.std(trainX)\n",
    "# trainX = (trainX - trainMean)/trainStd\n",
    "# validX = (validX - trainMean)/trainStd\n",
    "# testX = (testX - trainMean)/trainStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "trainX = ss.fit_transform(trainX)\n",
    "validX = ss.transform(validX)\n",
    "testX = ss.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNeuralNetwork(N_inputs=784, N_outputs=10, N_layers=4, Layer_sizes=[256, 128, 64, 32], activation=\"tanh\", learning_rate=1, weight_init=\"normal\", num_epochs=150, batch_size=len(X)//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.2301526275949474, Score: 11.27142857142857\n",
      "Epoch: 1, Loss: 0.23015049633412601, Score: 11.27142857142857\n",
      "Epoch: 2, Loss: 0.23014834752839708, Score: 11.27142857142857\n",
      "Epoch: 3, Loss: 0.230146167215418, Score: 11.27142857142857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_20405/1370029121.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_20405/2239731862.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, validX, validY, logs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_oht\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_oht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_20405/2239731862.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcurrentActivationFuntion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivationOutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Z'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dW'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivationOutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             self.gradients['db' + str(i)] = (1/len(X)) * np.sum(\n",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_20405/2239731862.py\u001b[0m in \u001b[0;36mtanh_backward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtanh\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \"\"\"\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;31m# return 1-X**2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_20405/2239731862.py\u001b[0m in \u001b[0;36mtanh_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# return (np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtanh_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainLoss, validLoss = model.fit(trainX, trainY, validX=validX, validY=validY, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.saveWeights(\"Weights/Ques2/weights.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MyNeuralNetwork(N_inputs=784, N_outputs=10, N_layers=1, Layer_sizes=[\n",
    "                        100], activation=\"sigmoid\", learning_rate=0.01, weight_init=\"normal\", num_epochs=2, batch_size=len(X)//20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.loadWeights(\"Weights/Ques2/weights.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.18715989664770388, Score: 11.287755102040817\n",
      "Epoch: 1, Loss: 0.20056223072583163, Score: 11.26530612244898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18715989664770388, 0.20056223072583163]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(trainX, trainY, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.18715989664770388, Score: 11.287755102040817\n",
      "Epoch: 1, Loss: 0.20056223072583163, Score: 11.26530612244898\n"
     ]
    }
   ],
   "source": [
    "_, _ = model1.fit(trainX, trainY, validX=validX, validY=validY, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_3349/1653368765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Weights/Ques2/weights.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fc/z3ktrz354nddfg1wt432tbm80000gn/T/ipykernel_3349/684942059.py\u001b[0m in \u001b[0;36mloadWeights\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    440\u001b[0m                                          pickle_kwargs=pickle_kwargs)\n\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[1;32m    728\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "model.loadWeights(\"Weights/Ques2/weights.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.37142857142857"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyG0lEQVR4nO3deXxU9b3/8ddnZpLJCoQkQEhYgoDIGiBgXaqg1mL1Qqu4ULxK7a3WW2tre91uW7VWf7W3/q69/n6t1bbWW2tFaysXK5aqdfvVjYjIjiIECFsCCVlIMplJPr8/zkkYwoRMIMnJ8nk+HvOYM9/zPWc+cyB55yxzvqKqGGOM6X98XhdgjDHGGxYAxhjTT1kAGGNMP2UBYIwx/ZQFgDHG9FMBrwvoiKysLB09erTXZRhjTK/ywQcfHFDV7NbtvSoARo8eTVFRkddlGGNMryIiO2K12yEgY4zppywAjDGmn7IAMMaYfqpXnQMwxnSPcDhMSUkJ9fX1XpdiOiApKYm8vDwSEhLi6m8BYIw5RklJCenp6YwePRoR8bocEwdV5eDBg5SUlJCfnx/XMnYIyBhzjPr6ejIzM+2Xfy8iImRmZnZor80CwBgTk/3y7306+m8WVwCIyDwR2SIiW0XkjhjzvyMiG0VkrYi8KiKj3PYCEXlHRDa4866MWuYJEdkuImvcR0GHKu+ANS8+yofLHqa2uryr3sIYY3qddgNARPzAz4GLgInAIhGZ2Krbh0Chqk4FngP+w22vBa5R1UnAPOBnIjIoarlbVbXAfaw5qU9yHLruT0xf8wPkwVN55/Hbu+ptjDGd5ODBgxQUFFBQUMCwYcPIzc1ted3Q0HDcZYuKirj55pvbfY8zzzyzU2p9/fXXueSSSzplXd0tnpPAs4GtqroNQESWAguAjc0dVPW1qP7vAle77R9H9dkjIqVANnDopCvvgGm3/pX1Ra/R+NqPmbXjMfaV3MCwvDHdWYIxpgMyMzNZs2YNAPfccw9paWn827/9W8v8SCRCIBD711dhYSGFhYXtvsfbb7/dKbX2ZvEcAsoFdkW9LnHb2vJV4KXWjSIyG0gEPo1qvt89NPSQiARjrUxErheRIhEpKisri6PcY/n8Piaffj7ZVzxMQJr49G+PntB6jDHeWbJkCV//+tc5/fTTue2223j//fc544wzmD59OmeeeSZbtmwBjv6L/J577uG6665jzpw5jBkzhocffrhlfWlpaS3958yZw8KFC5kwYQKLFy+meaTEFStWMGHCBGbOnMnNN9/cob/0n376aaZMmcLkyZO5/XbnyENjYyNLlixh8uTJTJkyhYceegiAhx9+mIkTJzJ16lSuuuqqk99YcerUy0BF5GqgEDi3VXsO8CRwrao2uc13AvtwQuEx4Hbg3tbrVNXH3PkUFhae1PiVw8dMZEPSdPJ3/olI5P42/4Iwxhzxwxc2sHFPVaeuc+LwAdz9T5M6vFxJSQlvv/02fr+fqqoq3nrrLQKBAK+88gr//u//zp/+9Kdjltm8eTOvvfYa1dXVnHrqqdx4443HXCf/4YcfsmHDBoYPH85ZZ53FP/7xDwoLC7nhhht48803yc/PZ9GiRXHXuWfPHm6//XY++OADMjIyuPDCC1m2bBkjRoxg9+7drF+/HoBDhw4B8MADD7B9+3aCwWBLW3eIZw9gNzAi6nWe23YUEbkA+B4wX1VDUe0DgBeB76nqu83tqrpXHSHgtziHmrpcY8E1DKeMNW883x1vZ4zpRJdffjl+vx+AyspKLr/8ciZPnswtt9zChg0bYi5z8cUXEwwGycrKYsiQIezfv/+YPrNnzyYvLw+fz0dBQQHFxcVs3ryZMWPGtFxT35EAWLVqFXPmzCE7O5tAIMDixYt58803GTNmDNu2beOb3/wmf/3rXxkwYAAAU6dOZfHixfz+97/v1j9M43mnVcA4EcnH+cV/FfDl6A4iMh14FJinqqVR7YnA88DvVPW5VsvkqOpeca5b+iKw/mQ+SLwmzl1Exbt3o0VPwPmXd8dbGtOrnchf6l0lNTW1ZfoHP/gBc+fO5fnnn6e4uJg5c+bEXCYYPHJ02e/3E4lETqhPZ8jIyOCjjz5i5cqV/PKXv+TZZ5/l8ccf58UXX+TNN9/khRde4P7772fdunXdEgTt7gGoagS4CVgJbAKeVdUNInKviMx3u/0USAP+6F7SudxtvwI4B1gS43LPp0RkHbAOyALu67RPdRyBYDKfDrmASbWrCDWE2l/AGNMjVVZWkpvrnI584oknOn39p556Ktu2baO4uBiAZ555Ju5lZ8+ezRtvvMGBAwdobGzk6aef5txzz+XAgQM0NTVx2WWXcd9997F69WqamprYtWsXc+fO5Sc/+QmVlZXU1NR0+ueJJa6IUdUVwIpWbXdFTV/QxnK/B37fxrzz4i+zcyWMOoOU0j+zedNqJkw7w6syjDEn4bbbbuPaa6/lvvvu4+KLL+709ScnJ/OLX/yCefPmkZqayqxZs9rs++qrr5KXl9fy+o9//CMPPPAAc+fORVW5+OKLWbBgAR999BFf+cpXaGpyToX++Mc/prGxkauvvprKykpUlZtvvplBgwZ1+ueJRZrPdvcGhYWF2hkDwpQWb2DIE2fyj9Pu4qwrv9sJlRnTt2zatInTTjvN6zI8V1NTQ1paGqrKN77xDcaNG8ctt9zidVnHFevfTkQ+UNVjro3tl7eCGDJqIlWkIntWe12KMaYH+9WvfkVBQQGTJk2isrKSG264weuSOlX/vA5ShF3Jp5Fd3S3nnY0xvdQtt9zS4//iPxn9cg8AoG7IdPIbd1JeUeF1KcYY44l+GwCpY2YTkCaK17/jdSnGGOOJfhsAI6ecDUDVp+95XIkxxnij3wZA6uDh7JchJJd+6HUpxhjjiX4bAAD7B0wir3Zj+x2NMd1q7ty5rFy58qi2n/3sZ9x4441tLjNnzhyaLxP/whe+EPOeOvfccw8PPvjgcd972bJlbNx45PfCXXfdxSuvvNKB6mPribeN7tcB0DD4VHL0AJVV1V6XYoyJsmjRIpYuXXpU29KlS+O+H8+KFStO+MtUrQPg3nvv5YILYn7Xtdfr1wGQmH0KPlH27djsdSnGmCgLFy7kxRdfbBn8pbi4mD179vDZz36WG2+8kcLCQiZNmsTdd98dc/nRo0dz4MABAO6//37Gjx/P2Wef3XLLaHCu8Z81axbTpk3jsssuo7a2lrfffpvly5dz6623UlBQwKeffsqSJUt47jnnVmavvvoq06dPZ8qUKVx33XWEQqGW97v77ruZMWMGU6ZMYfPm+H+neHnb6P75PQDXoLxT4X04tPtjmNL217yN6ddeugP2revcdQ6bAhc90ObswYMHM3v2bF566SUWLFjA0qVLueKKKxAR7r//fgYPHkxjYyPnn38+a9euZerUqTHX88EHH7B06VLWrFlDJBJhxowZzJw5E4BLL72Ur33tawB8//vf5ze/+Q3f/OY3mT9/PpdccgkLFy48al319fUsWbKEV199lfHjx3PNNdfwyCOP8O1vfxuArKwsVq9ezS9+8QsefPBBfv3rX7e7Gby+bXS/3gMYMsr5unSodKvHlRhjWos+DBR9+OfZZ59lxowZTJ8+nQ0bNhx1uKa1t956iy996UukpKQwYMAA5s+f3zJv/fr1fPazn2XKlCk89dRTbd5OutmWLVvIz89n/PjxAFx77bW8+eabLfMvvfRSAGbOnNlyA7n2eH3b6H69B5A0IJsaUvBVFHtdijE913H+Uu9KCxYs4JZbbmH16tXU1tYyc+ZMtm/fzoMPPsiqVavIyMhgyZIl1NfXn9D6lyxZwrJly5g2bRpPPPEEr7/++knV23xL6c64nXR33Ta6X+8BIEJpYDiph3d6XYkxppW0tDTmzp3Ldddd1/LXf1VVFampqQwcOJD9+/fz0kvHjD57lHPOOYdly5ZRV1dHdXU1L7zwQsu86upqcnJyCIfDPPXUUy3t6enpVFcfe2HIqaeeSnFxMVu3OkcMnnzySc4999xj+nWE17eN7td7AAA1KSMYXGUngY3piRYtWsSXvvSllkNB06ZNY/r06UyYMIERI0Zw1llnHXf5GTNmcOWVVzJt2jSGDBly1C2df/SjH3H66aeTnZ3N6aef3vJL/6qrruJrX/saDz/8cMvJX4CkpCR++9vfcvnllxOJRJg1axZf//rXO/R5etpto/vl7aCjrX78W0ze8SThO/aQmpzUqes2prey20H3Xp1+O2gRmSciW0Rkq4jcEWP+d0Rko4isFZFXRWRU1LxrReQT93FtVPtMEVnnrvNhd2jIbhfIGkuiNLJnh50INsb0L+0GgIj4gZ8DFwETgUUiMrFVtw+BQlWdCjwH/Ie77GDgbuB0nEHf7xaRDHeZR4CvAePcx7yT/jQnID1nHAAVJVva6WmMMX1LPHsAs4GtqrpNVRuApcCC6A6q+pqq1rov3wWaD3J9HnhZVctVtQJ4GZgnIjnAAFV9V51jUL/DGRi+22WPmgBAnV0KasxRetPhYePo6L9ZPAGQC+yKel3itrXlq0Dzqfm2ls11p9tdp4hcLyJFIlJUVlYWR7kdk5Y1khAJUL6t09dtTG+VlJTEwYMHLQR6EVXl4MGDJCXFfy6zU68CEpGrgULg5K6NiqKqjwGPgXMSuLPW28Lno9Q/jOQauxTUmGZ5eXmUlJTQFX90ma6TlJR01FVG7YknAHYDI6Je57ltRxGRC4DvAeeqaihq2Tmtln3dbc9r1X7MOrtLVXIeGYc9e3tjepyEhATy8/O9LsN0sXgOAa0CxolIvogkAlcBy6M7iMh04FFgvqqWRs1aCVwoIhnuyd8LgZWquheoEpHPuFf/XAP8Tyd8nhMSSh/F8KZ9NIQbvSrBGGO6XbsBoKoR4CacX+abgGdVdYOI3CsizTfW+CmQBvxRRNaIyHJ32XLgRzghsgq4120D+Ffg18BW4FOOnDfodr5BI0mVEGVl+7wqwRhjul1c5wBUdQWwolXbXVHTbd4sW1UfBx6P0V4ETI670i4UzHKOcJXv2Ubu8OOd3zbGmL6jf98LyJU+1DnWWVNW7G0hxhjTjSwAgMzhYwBoOLirnZ7GGNN3WAAAyYNyCOOHypL2OxtjTB9hAQDg83HAl0Vi7V6vKzHGmG5jAeCqShxKWv1+r8swxphuYwHgqkvOYXBjafsdjTGmj7AAcDWlD2eIllNTF2q/szHG9AEWAC5/xggSpJHSvXZPIGNM/2AB4ErJcsawObTX7gpqjOkfLABcA4Y5XwarPWB7AMaY/sECwJWZ4wRApMK+DGaM6R8sAFyB1AwOk4Svym4LbYzpHywAmolQ7s8maF8GM8b0ExYAUWqCQxnYYF8GM8b0DxYAUUKpw8lsLLNxUI0x/UJcASAi80Rki4hsFZE7Ysw/R0RWi0hERBZGtc91B4hpftSLyBfdeU+IyPaoeQWd9aFOlKbnki2VHKys9roUY4zpcu0GgIj4gZ8DFwETgUUiMrFVt53AEuAP0Y2q+pqqFqhqAXAeUAv8LarLrc3zVXXNiX6IzpKQORKAA3u2e1yJMcZ0vXj2AGYDW1V1m6o2AEuBBdEdVLVYVdcCTcdZz0LgJVWtPeFqu1hqtvNlsKp9xd4WYowx3SCeAMgFoi+OL3HbOuoq4OlWbfeLyFoReUhEgrEWEpHrRaRIRIrKyspO4G3jl5HjDAxTd9C+DGaM6fu65SSwiOQAU3AGlm92JzABmAUMBm6PtayqPqaqhapamJ2d3aV1Dhzq7AHoIfsymDGm74snAHYDI6Je57ltHXEF8LyqhpsbVHWvOkLAb3EONXlKElM4xAD81Xu8LsUYY7pcPAGwChgnIvkikohzKGd5B99nEa0O/7h7BYiIAF8E1ndwnV3iUMIQkuv3eV2GMcZ0uXYDQFUjwE04h282Ac+q6gYRuVdE5gOIyCwRKQEuBx4VkQ3Ny4vIaJw9iDdarfopEVkHrAOygPs64fOctJqkYQwK25fBjDF9XyCeTqq6AljRqu2uqOlVOIeGYi1bTIyTxqp6XkcK7S7htOGMrFpNQ6SJxIB9T84Y03fZb7hWfANzGSC1lHbxFUfGGOM1C4BWgpnOlUDlNjCMMaaPswBoJW3IaABqSnd4W4gxxnQxC4BWMnOdL4OF7Mtgxpg+zgKgleSMXBoRqCrxuhRjjOlSFgCt+QOUSyYJNTYwjDGmb7MAiKEycShpIfsymDGmb7MAiKEueRgZkVKvyzDGmC5lARBDY/pwhulBaurD7Xc2xpheygIghsDgUQQlzJ7ddimoMabvsgCIIXXYKQCU7/7E40qMMabrWADEkJk3HoC6/Z96XIkxxnQdC4AY0oc6XwbTchsb2BjTd1kAxCCJKRyQwSRU2chgxpi+ywKgDRXBXNLrOzrwmTHG9B4WAG2oSx3BkMhemprU61KMMaZLxBUAIjJPRLaIyFYRuSPG/HNEZLWIRERkYat5jSKyxn0sj2rPF5H33HU+4w432WPooJEMo5yyQ1Vel2KMMV2i3QAQET/wc+AiYCKwSEQmtuq2E1gC/CHGKupUtcB9zI9q/wnwkKqOBSqAr55A/V0mmH0KPlH277JLQY0xfVM8ewCzga2quk1VG4ClwILoDqparKprgaZ43tQdCP484Dm36b9xBobvMdJzxgJQtccCwBjTN8UTALlA9OUwJcQY4/c4kkSkSETeFZEvum2ZwCF3wPnjrlNErneXLyrrxmEas0Y63wUIldmloMaYvimuQeFP0ihV3S0iY4C/i8g6oDLehVX1MeAxgMLCwm47IxscOJwQifgq7XYQxpi+KZ49gN3AiKjXeW5bXFR1t/u8DXgdmA4cBAaJSHMAdWid3cLnoywwjOQa+y6AMaZviicAVgHj3Kt2EoGrgOXtLAOAiGSISNCdzgLOAjaqqgKvAc1XDF0L/E9Hi+9q1UnDyWjY43UZxhjTJdoNAPc4/U3ASmAT8KyqbhCRe0VkPoCIzBKREuBy4FER2eAufhpQJCIf4fzCf0BVN7rzbge+IyJbcc4J/KYzP1hnaEgfSU7TPuobIu13NsaYXiaucwCqugJY0artrqjpVTiHcVov9zYwpY11bsO5wqjH8mXmM2BvHZ/sLmFc/mivyzHGmE5l3wQ+jgF5pwFQWrze40qMMabzWQAcx7BTpgFQt3tDOz2NMab3sQA4jmDmaOpJxHfgY69LMcaYTmcBcDw+H/sTRpBes83rSowxptNZALSjOv0UcsI7abS7ghpj+hgLgHZo1njy5AC793ffbSiMMaY7WAC0IyXXufHpvm3rPK7EGGM6lwVAO7LzpwJQU2JXAhlj+hYLgHYMGH4qEfxomV0JZIzpWywA2hNIZL9/OKlVW72uxBhjOpUFQBwOpeUztGEHzj3sjDGmb7AAiENk8DhG6D5KD1V7XYoxxnQaC4A4pOROJiBN7Pj4I69LMcaYTmMBEIecCacDUPnpKo8rMcaYzmMBEIe04ROoIwn//rVel2KMMZ3GAiAePj+7k8eRVbXJ60qMMabTxBUAIjJPRLaIyFYRuSPG/HNEZLWIRERkYVR7gYi8IyIbRGStiFwZNe8JEdkuImvcR0GnfKIuUpc5mbFN26morvO6FGOM6RTtBoCI+IGfAxcBE4FFIjKxVbedwBLgD63aa4FrVHUSMA/4mYgMipp/q6oWuI81J/QJuklw5ExSJMS2zau9LsUYYzpFPHsAs4GtqrpNVRuApcCC6A6qWqyqa4GmVu0fq+on7vQeoBTI7pTKu1nOhM8AcOjTIo8rMcaYzhFPAOQCu6Jel7htHSIis4FE4NOo5vvdQ0MPiUiwjeWuF5EiESkqK/PujpzpeROpI4jss0tBjTF9Q7ecBBaRHOBJ4Cuq2ryXcCcwAZgFDAZuj7Wsqj6mqoWqWpid7eHOg8/PnqSxZFVt9K4GY4zpRPEEwG5gRNTrPLctLiIyAHgR+J6qvtvcrqp71RECfotzqKlHO5w5mVMat1F5uN7rUowx5qTFEwCrgHEiki8iicBVwPJ4Vu72fx74nao+12pejvsswBeB9R2o2xNJI2eSKiE+2Wgngo0xvV+7AaCqEeAmYCWwCXhWVTeIyL0iMh9ARGaJSAlwOfCoiDTfPP8K4BxgSYzLPZ8SkXXAOiALuK8zP1hXyJ06B4DyTW96W4gxxnSCQDydVHUFsKJV211R06twDg21Xu73wO/bWOd5Haq0B0gdNp4KGUTS3ve8LsUYY06afRO4I0TYM2gGp9SupSHS1H5/Y4zpwSwAOkhGnUGuHGDLFhsi0hjTu1kAdNCwKecDULr+NY8rMcaYk2MB0EGD8wuoJhX/rnfb72yMMT2YBUBH+fyUpE9lZM0amppsiEhjTO9lAXACwnlnMIbdfLpju9elGGPMCbMAOAE5Uy8AYGfRXz2uxBhjTpwFwAnIPvUzVEo6idv/7nUpxhhzwiwAToTPz66MzzDh8PvUhcJeV2OMMSfEAuAEBcZfSLZUsn71//O6FGOMOSEWACdo9On/BEDlOjsPYIzpnSwATlBSRg7FCeMYst9uDGeM6Z0sAE7CodxzmBjZTMnefV6XYowxHWYBcBKyp19CQJrY+vYyr0sxxpgOswA4CblTzqVcBhH85C9el2KMMR0WVwCIyDwR2SIiW0XkjhjzzxGR1SISEZGFreZdKyKfuI9ro9pnisg6d50PuyOD9S4+P7uGns/Uuvc5UFHhdTXGGNMh7QaAiPiBnwMXAROBRSIysVW3ncAS4A+tlh0M3A2cjjPm790ikuHOfgT4GjDOfcw74U/hoUEzF5IqITa9tczrUowxpkPi2QOYDWxV1W2q2gAsBRZEd1DVYlVdC7QeJeXzwMuqWq6qFcDLwDx3POABqvquqirwO5xxgXudkTM+xyHS8W9+wetSjDGmQ+IJgFxgV9TrErctHm0tm+tOn8g6exTxJ7Ajaw5TDr9NZXWN1+UYY0zcevxJYBG5XkSKRKSorKzM63JiSpt+KelSx7o3/ux1KcYYE7d4AmA3MCLqdZ7bFo+2lt3N0YPIt7lOVX1MVQtVtTA7OzvOt+1eY06/hEMMIGH9s16XYowxcYsnAFYB40QkX0QSgauA5XGufyVwoYhkuCd/LwRWqupeoEpEPuNe/XMN8D8nUH+PIIFEtudcREHdO+zbv8frcowxJi7tBoCqRoCbcH6ZbwKeVdUNInKviMwHEJFZIlICXA48KiIb3GXLgR/hhMgq4F63DeBfgV8DW4FPgZc69ZN1s2HnfIWgRNjyypNel2KMMXER5yKc3qGwsFCLioq8LiM2VXbeN40akjjt++/RG7/WYIzpm0TkA1UtbN3e408C9xoiHBh7KRMbt/Dxhg+9rsYYY9plAdCJxp7/VSLqY/8bj3ldijHGtMsCoBMNGDKCDQPPYWrpC1RXV3ldjjHGHJcFQCdLPesGBkkNa1c+7nUpxhhzXBYAnWzs7IvY6R9J9qYn0abWd8YwxpiewwKgs4lQNuFqxjduZcOq17yuxhhj2mQB0AUmzrueapKpe/O/vC7FGGPaZAHQBZLTM9iUewUza95kx8cfeV2OMcbEZAHQRcbNv5UwAfa99FOvSzHGmJgsALpIxtARrM2+mOnlL1G2Z4fX5RhjzDEsALpQ7hdux08jny673+tSjDHmGBYAXWj4mImszvgCM/b/if07P/G6HGOMOYoFQBcbcekPAdj5/N0eV2KMMUezAOhiw0aOY/WQS5lRvoKST+yKIGNMz2EB0A1OWXgX9QQ5+PztXpdijDEtLAC6QfbQEawZ8zWm1b7Dhjee87ocY4wB4gwAEZknIltEZKuI3BFjflBEnnHnvycio932xSKyJurRJCIF7rzX3XU2zxvSmR+sp5l55ffYKcMZ+MYPiITqvC7HGGPaDwAR8QM/By4CJgKLRGRiq25fBSpUdSzwEPATAFV9SlULVLUA+Gdgu6quiVpucfN8VS096U/TgyUlJVN29g/Ja9rDmmd/5HU5xhgT1x7AbGCrqm5T1QZgKbCgVZ8FwH+7088B58uxYyIucpftt2acdzmrUj7L1K2PsvcTGzXMGOOteAIgF9gV9brEbYvZxx1EvhLIbNXnSuDpVm2/dQ///CBGYAAgIteLSJGIFJWVlcVRbs8lIoy4+hccJpnDz96ANoa9LskY0491y0lgETkdqFXV9VHNi1V1CvBZ9/HPsZZV1cdUtVBVC7Ozs7uh2q41bPhINhR8n7HhLax5xg4FGWO8E08A7AZGRL3Oc9ti9hGRADAQOBg1/ypa/fWvqrvd52rgDziHmvqFM+dfz7vJ5zJ5y/9l50eve12OMaafiicAVgHjRCRfRBJxfpkvb9VnOXCtO70Q+LuqKoCI+IAriDr+LyIBEclypxOAS4D19BM+v49Tvvpr9ksmwWX/Qn3VwfYXMsaYTtZuALjH9G8CVgKbgGdVdYOI3Csi891uvwEyRWQr8B0g+lLRc4Bdqrotqi0IrBSRtcAanD2IX53sh+lNsrOGUPr5X5LRVM72x76MNka8LskY08+I+4d6r1BYWKhFRUVel9GpXv7d/+Jz237CulHXMuUrD3tdjjGmDxKRD1S1sHW7fRPYY+dffSd/H7CAKTv+m4//+kuvyzHG9CMWAB7z+YTZX3+UDwIFjHnnTna8bbeKMMZ0DwuAHiAtJZncG57jE18+Q/92I/vWvup1ScaYfsACoIcYlp1NcMky9pDNgD9/mdK1r3hdkjGmj7MA6EHGjBpJw+Jl7CWTAX9exP41K70uyRjTh1kA9DATxo2nYfFydjGUjGVfpuSt33tdkjGmj7IA6IFOGzcWlqxgg4wn79VvsGP5j6EXXa5rjOkdLAB6qHGjRzLkGyt4I3AWo1Y/QPEjC9H6Kq/LMsb0IRYAPVhuVgbTv7uMP2Zcz4j9r1L6n2dSv7vf3DHDGNPFLAB6uAHJiVz2zf9g+bRH8IWq4FfnceAfv7NDQsaYk2YB0Av4fMKXLl3EJ198kY3kk/XyNyl55Ito1R6vSzPG9GIWAL3ImdOnkH3Tyzw14F/I2v8Pah8qpPytX9vegDHmhFgA9DIjsgaw6NsPsvKcP7OxaSSDX/0u+/7P52jcs9br0owxvYwFQC/k8wkLzj+HnG+9wuODbiZ4cBPy2Dnsf/JfoGqv1+UZY3oJC4BeLG9wGl/51r2smv8qzwTmM2jr84QeKqDixbuhrsLr8owxPZyNB9BH1Icbee6Vt8h89wEukneo9yUTKvgKA8+7BdKGeF2eMcZDJzUegIjME5EtIrJVRO6IMT8oIs+4898TkdFu+2gRqRORNe7jl1HLzBSRde4yD4uInMTn6/eSEvxcfdEcZv7bMh457Xe8EplO2gePEP7fk6j807fgwCdel2iM6WHa3QMQET/wMfA5oARnjOBFqroxqs+/AlNV9esichXwJVW90g2Cv6jq5BjrfR+4GXgPWAE8rKovHa8W2wOIX2l1Pc/97Q2GfPQI8+VNEqWRimFnMejcG5HxF4E/4HWJxphucjJ7ALOBraq6TVUbcAZ3X9CqzwLgv93p54Dzj/cXvYjkAANU9V138PjfAV+MoxYTpyHpSfzrZZ9nzm3P8PjsF/mFbxF1ezchz1zN4Z9OIvS3H9pegTH9XDx/BuYCu6JelwCnt9VHVSMiUglkuvPyReRDoAr4vqq+5fYvabXO3FhvLiLXA9cDjBw5Mo5yTbSstCBfv/gMQp+fzYsf7mLTG89wduVfOPsfD8Hb/0l1VgGpsxbjm7gA0od6Xa4xpht19XGAvcBIVT0oIjOBZSIyqSMrUNXHgMfAOQTUBTX2C8GAn0tnjUYLb+Ojkhv46TsfEtj4HJeUvsGEl25FX7qN2iEzSJn2RWTCxZB5itclG2O6WDwBsBsYEfU6z22L1adERALAQOCge3gnBKCqH4jIp8B4t39eO+s0XUBEKBgxiIIRc6kPn8PK9Xt58r1/kF3yNy7YV8Tk0h/Ayz+gbtCpJE2Zj4y/EIbPsHMGxvRB8ZwEDuCcBD4f55f0KuDLqrohqs83gClRJ4EvVdUrRCQbKFfVRhEZA7zl9iuPcRL4/6jqiuPVYieBu05lbZiXN+3n/dUfMmDn37hAVjHLtwU/TYQDaTD6bBLGnQdj5kDWeLCLtozpNdo6CRzX9wBE5AvAzwA/8Liq3i8i9wJFqrpcRJKAJ4HpQDlwlapuE5HLgHuBMNAE3K2qL7jrLASeAJKBl4BvajvFWAB0j6r6MH/fVMobH22B7W9R2PgRZ/vXM0r2AxBOGYo//2x8I0+HEbNh6BTbQzCmBzupAOgpLAC6X7ixiQ93HuLNj8vYvGkdmWXvcrZvHbN9HzNUygFoDCQjuYVOIOTOhJypMCDX9hKM6SEsAEynOFgT4v9tPcC728op3raFzPI1zPR9zCz/J5wmxfhpAqAxeTC+nGlIzlQYNhVypsHgU8Bndx8xprtZAJgucbAmRNGOCt7fXs7a7Xtg33omsJ1JUkxBYAdj2UWACABNCan4hk6C7PGQdSpkT3CmB460YDCmC1kAmG5RH25k875q1pUcYm1JJZtKDqBlm5koxUyUHUxJKGGc7GZg06GWZTSQjGSNg+xTnRPMGfkwON95Thlsh5KMOUkWAMYztQ0RNu6p4qOSSjbtreLj/dXs37+XvMguxvl2M1Z2MzlxP+OkhMzG0qOW1eAAJGNUVCiMdqYzRjvnGQKJnnwmY3qTtgLALt0wXS4lMUDh6MEUjh7c0tbYpOwqr2XL/mo+3lfNk+7z3gPl5GgpI2U/o6SUsZQx/uABRpR/SObmlwhoOGrNAmlDYWCuEwYD8448N0+nDbXDS8a0wQLAeMLvE0ZnpTI6K5XPTxrW0h5ubKKkoo7iA4fZduAwmw4cZsWBw2w/cJh99YcZquWM9JUyQkoZk1DBKQ2HyCsvJ/vgRwwKv0xCY93Rb+QLOCGQNhTShzm3xk4b5tz2Im3okenUIbY3YfodCwDToyT4feRnpZKflcrcVvPqw43sLK9lW9lhig8eZndFHe9X1FJSUUdJRR114QgDOcxwOUiOHGRM4iHGJR4ir7GK7OpDZFR+Qnr4XYINFQgxDn0mD3ZCIjULUrIgJdOdzmw1neWcm/AndMs2MaarWACYXiMpwc/4oemMH5p+zDxVpfxwQ0sYlFTUsvtQHSsr6thbWc/+qnrKDzcAECBCJlVkyyFGBKo4JeUwIxOrGe6vZEj4EAPLD5FatpNgQwUJDZXHKWhgq6AYDEmDIHmQ+5zhTrvPyRnOMj5/F2wdYzrOAsD0CSJCZlqQzLQg00YMitmnPtxIaVWIfVX17KuqZ3+l87y9qp53KuvZV1lPaXU94cYjewd+Gsmghmx/NaOT6xiZVMfwhFqGBmrI8lWRQTXp9ZWk1mwjMbyaQKgSidTFfP8WwQFRITHo6LAIDnAf6ZDkPgfTj24PBO3KKNMpLABMv5GU4GdkZgojM1Pa7NPUpFTWhTlQE6KsOkRZTYgDNQ0cqAlxoDrExzUh3q5p4EBViAM1oaPColkiYXIS68lLDjE8GCInWE92oI4sfy0ZvloGUkOa1pDaVEOwtorEyr0EQlVI/SFoDLX/QXwJrcJh4JHpY0IjHRJSIDEVEtPc56hHQoqFST9mAWBMFJ9PyEhNJCM1kXExDjVFU1Wq6iJuSIQ4WNNARW0DFYcbqKgNU1HbQFltAx8fbqCiMkzF4QaqQ5E215eU4CMrCXKSwgxJbGBIYgOZCSEyAyEG+esZIHWk++pIo46UpsMkNdUSbDxMQqQaf1UJUl8FoWoIVUFT2+9zNDk6DGKFRMsjLUa/FEhIhYQkpz0h2X2kgD/RwqWHswAw5gSJCANTEhiYksDYIWlxLRNubOKQGw5OUDhhUX64gUO1DVTWhamqi1BRH6a4LkxVZZjK2jDVoQjH+8qOTyA9KYGByQkMGOQnK1kZmtBAZkKYgYEGBgbCDPSFSPOFSJMQqVJPMvUkaT1JTXUkNtWR0FSHL1wLDTVQfwiqdkOD+7rhcHx7J0dvoahQaH5OOrYt0LotRpgEWrUFghBofk5yTshb2HSYBYAx3SjB7yM7PUh2erBDyzU1KdWhCFV1YSck6p2gqHKnneBonhehsi5MSTXU1PuoCfmpCSUAbR/6apaU4CMtmEBa0E9aUoC01ABpgxNITwowIBEGBcJkJIQZ6G8gTepJ94VIljAp0kASIZJpIJEQQQ2R0FSPL1IP4Tr3Ues8R+qcPZWaUret/si8DodMM3GCoDkQmp8TkmK3tzxHP2L0SUg+/rL+ROe1L9ArA8gCwJhewOcTBiY7f+GPaL/7MZqalNpwIzX1EWpCYarrI9SEIu7ro6er3enD7vSeQ3VH9WlobIpac8B9JMd836QEHymJAVIS/aQmBkgJ+klJ9JOSGCA1zU9KMEBKgvOcmugnNVFI84VJD0RI84VJ8zWQTANJ0kASDQQ1RKLWE2hqQBpDEAlBpN4JkUj9kdctz1Htoeqj54fr3Nd1oE0x64+fOEHgDzrfJ4n5HDwSGMc8x9F/7AXOOZ5OZAFgTD/g8wlpwQBpwQCQdFLrCkWcIKltaKS2oZHDDRHqGho5HIq0et1IbUOEww0RakNH+tY2NHKwprZl+Vq3LT4BIA2fQHKCn+REP0kJ/mOmkxJ8TltS1PwY/Z2+fpIDSoovQoqESfKFSaKBJMIEtOHYUAm3CpZGt09jCCINrZ6j5ze4h9KO0/94525uKrIAMMZ4KxjwE0zzk9mJ62xqUurCRwLhSHg0UtcQoT7cRF24kbqGRurCjdRHTbd+faguTH1l1LyGRuojjTGv2GpPgl9IckMiGAiSlJBCMOBzX/vceT6CgSPPwQQfwaSj25Pc9pbnqOVb1hPwEwxAkDDS2HB0cERCMGhkJ25xR1wBICLzgP/CGRHs16r6QKv5QeB3wEzgIHClqhaLyOeAB4BEoAG4VVX/7i7zOpADNF80faGqHn0nMGNMv+DzCanBAKnBANCx8yPxCjc2OUERbqS+oaklIOoaGlva2w6YJkKRRkLuc33YWVd1fYT6cCOhiPO6eToUOblDSokBH0kBH8GogPnNtaMYldm526bdABARP/Bz4HNACbBKRJar6saobl8FKlR1rDsm8E+AK4EDwD+p6h4RmQysBHKjllusqnZ7T2NMl0vw+0jw+0hP6vpbeKiqEwTRgeEGSMtzuPHYtlbPzcuGIo0kJXT+N8jj2QOYDWxV1W0AIrIUWABEB8AC4B53+jng/4qIqOqHUX02AMkiElTVEz3Vb4wxPZ7IkUNH0HPvGRXPfXJzgV1Rr0s4+q/4o/qoagSohGMOEV4GrG71y/+3IrJGRH4gEvsaKhG5XkSKRKSorKwsjnKNMcbEo1tulC4ik3AOC90Q1bxYVacAn3Uf/xxrWVV9TFULVbUwOzu764s1xph+Ip4A2A1HXXqc57bF7CMiAWAgzslgRCQPeB64RlU/bV5AVXe7z9XAH3AONRljjOkm8QTAKmCciOSLSCJwFbC8VZ/lwLXu9ELg76qqIjIIeBG4Q1X/0dxZRAIikuVOJwCXAOtP6pMYY4zpkHYDwD2mfxPOFTybgGdVdYOI3Csi891uvwEyRWQr8B3gDrf9JmAscJd7rH+NiAzBuc5rpYisBdbg7EH8qhM/lzHGmHbYoPDGGNPHtTUovI2WbYwx/ZQFgDHG9FO96hCQiJQBO05w8Sycbyb3ZFZj5+jpNfb0+sBq7Cw9pcZRqnrMdfS9KgBOhogUxToG1pNYjZ2jp9fY0+sDq7Gz9PQa7RCQMcb0UxYAxhjTT/WnAHjM6wLiYDV2jp5eY0+vD6zGztKja+w35wCMMcYcrT/tARhjjIliAWCMMf1UvwgAEZknIltEZKuI3NH+El1ezwgReU1ENorIBhH5lts+WEReFpFP3OeMHlCrX0Q+FJG/uK/zReQ9d1s+494g0Mv6BonIcyKyWUQ2icgZPW07isgt7r/zehF5WkSSvN6OIvK4iJSKyPqotpjbTRwPu7WuFZEZHtb4U/ffeq2IPO/ecLJ53p1ujVtE5PNe1Rg177siolE3vvRkOx5Pnw8AOTKk5UXARGCRiEz0tioiwHdVdSLwGeAbbk13AK+q6jjgVY7cVM9L38K5CWCznwAPqepYoAJnOFAv/RfwV1WdAEzDqbXHbEcRyQVuBgpVdTLOuNrNw6Z6uR2fAOa1amtru10EjHMf1wOPeFjjy8BkVZ0KfAzcCeD+/FwFTHKX+YX7s+9FjYjICOBCYGdUs1fbsW2q2qcfwBnAyqjXdwJ3el1Xqxr/B2fM5S1AjtuWA2zxuK48nF8E5wF/AQTnW42BWNvWg/oGAttxL2aIau8x25Ejo+UNxhmC9S/A53vCdgRGA+vb227Ao8CiWP26u8ZW874EPOVOH/VzjXP34jO8qhFnaNxpQDGQ5fV2bOvR5/cAiG9IS8+IyGhgOvAeMFRV97qz9gFDvarL9TPgNqDJfZ0JHFLnFuHg/bbMB8pwhhb9UER+LSKp9KDtqM7ARw/i/CW4F2e41A/oWduxWVvbraf+DF0HvORO95gaRWQBsFtVP2o1q8fU2Kw/BECPJSJpwJ+Ab6tqVfQ8df5E8OwaXRG5BChV1Q+8qiEOAWAG8IiqTgcO0+pwTw/YjhnAApywGg6kEuOQQU/j9XZrj4h8D+dQ6lNe1xJNRFKAfwfu8rqWePSHAIhnSMtu546E9iecXdg/u837RSTHnZ8DlHpVH3AWMF9EioGlOIeB/gsYJM6wn+D9tiwBSlT1Pff1cziB0JO24wXAdlUtU9Uw8GecbduTtmOztrZbj/oZEpElOKMILnaDCnpOjafghP1H7s9OHrBaRIbRc2ps0R8CIJ4hLbuViAjOKGqbVPU/o2ZFD615Lc65AU+o6p2qmqeqo3G22d9VdTHwGs6wn+B9jfuAXSJyqtt0PrCRHrQdcQ79fEZEUtx/9+Yae8x2jNLWdlsOXONexfIZoDLqUFG3EpF5OIcl56tqbdSs5cBVIhIUkXycE63vd3d9qrpOVYeo6mj3Z6cEmOH+X+0x27GFlycguusBfAHnioFPge/1gHrOxtm9bh4Sc41bYybOSddPgFeAwV7X6tY7B/iLOz0G5wdrK/BHIOhxbQVAkbstlwEZPW07Aj8ENuOMe/0kzpConm5H4GmccxJhnF9SX21ru+Gc/P+5+/OzDueKJq9q3IpzHL355+aXUf2/59a4BbjIqxpbzS/myElgT7bj8R52KwhjjOmn+sMhIGOMMTFYABhjTD9lAWCMMf2UBYAxxvRTFgDGGNNPWQAYY0w/ZQFgjDH91P8H9F4UME/OC/MAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainLoss, label='Training Loss')\n",
    "plt.plot(validLoss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_forward( X):\n",
    "    \"\"\"\n",
    "    Softmax activation function\n",
    "    X: input\n",
    "    return: output after applying the softmax function\n",
    "    \"\"\"\n",
    "    exp = np.exp(X - np.max(X))\n",
    "    return exp/np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "# softmax implemetatation incomplete\n",
    "def softmax_backward( X):\n",
    "    \"\"\"\n",
    "    Softmax activation function\n",
    "    X: input\n",
    "    return: output after applying the gradient of softmax function\n",
    "    \"\"\"\n",
    "    # softmax = self.softmax_forward(X)\n",
    "    # e = np.ones((softmax.shape[0], 1))\n",
    "    # v1 = np.dot(softmax, e.T)\n",
    "    # i = np.eye(softmax.shape[0])\n",
    "    # v2 = i - np.dot(e, softmax.T)\n",
    "    # return np.multiply(v1, v2)\n",
    "    X = np.array(X)\n",
    "    s = X.reshape(-1, 1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[10, 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.53978687e-05, 9.99954602e-01]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_forward(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -90, -200],\n",
       "       [-200, -380]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_backward(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
